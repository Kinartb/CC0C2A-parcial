{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-E-AFpjnYZi1",
        "BNtRZVEwPofF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1"
      ],
      "metadata": {
        "id": "J9wc3QbcYc8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTA: ES NECESARIO RECORRER ESTA CELDA PARA CONTINUAR CON LAS DEMAS EN PARTE 1\n",
        "\n",
        "# Suavizado Add-one (suavizado de Laplace): https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase2/Modelos-lenguaje1.ipynb\n",
        "# Suavizado Add-k: https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase2/Modelos-lenguaje2.ipynb\n",
        "# Suavizado Backoff:  https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase2/Counts-backoff-suavizado.ipynb\n",
        "# Suavizado Stupid Backoff: https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase2/Topicos-avanzados.ipynb\n",
        "\n",
        "corpus = \"all models are wrong. a model is wrong. some models are useful\"\n",
        "vocab = {'<s>','</s>','a', 'all','are','model','models','some','useful','wrong'}\n",
        "\n",
        "# Tokenizar\n",
        "tokens = corpus.lower().replace('.', '').split()\n",
        "tokens.insert(0, '<s>')\n",
        "tokens.append('</s>')\n",
        "\n",
        "# Frecuencias de bigramas y unigramas\n",
        "bigram_counts = {}\n",
        "unigram_counts = {}\n",
        "for i in range(len(tokens) - 1):\n",
        "  bigram = (tokens[i], tokens[i + 1])\n",
        "  if bigram in bigram_counts:\n",
        "    bigram_counts[bigram] += 1\n",
        "  else:\n",
        "    bigram_counts[bigram] = 1\n",
        "  if tokens[i] in unigram_counts:\n",
        "    unigram_counts[tokens[i]] += 1\n",
        "  else:\n",
        "    unigram_counts[tokens[i]] = 1\n",
        "\n",
        "# Calcular el total de palabras (tokens)\n",
        "total_words = sum(unigram_counts.values())\n",
        "\n"
      ],
      "metadata": {
        "id": "6vZ-C26eYYvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código tiene como objetivo procesar un corpus de texto para realizar un análisis de frecuencias de bigramas (pares de palabras consecutivas) y unigramas (palabras individuales).\n",
        "\n",
        "***Definición del Corpus y Vocabulario:***\n",
        "\n",
        "Se define un corpus de texto (corpus) y un conjunto de palabras para el vocabulario (vocab), lo cual es útil para limitar las palabras que se analizarán.\n",
        "\n",
        "***Tokenización:***\n",
        "\n",
        "Se convierte el corpus a minúsculas, se eliminan los puntos y se divide en palabras. Además, se insertan los tokens especiales `<s>` al inicio y `</s>` al final para marcar los límites de la frase.\n",
        "\n",
        "***Cálculo de Frecuencias***\n",
        "\n",
        "Se crean dos diccionarios: bigram_counts para almacenar la frecuencia de aparición de bigrams y unigram_counts para la frecuencia de unigrams.\n",
        "A través de un bucle for, el código recorre la lista de tokens para calcular:\n",
        "\n",
        "**Bigrams:** Pares consecutivos de palabras. Si un bigram ya existe en bigram_counts, su contador se incrementa en 1; si no, se crea una nueva entrada para ese bigram con un contador inicial de 1.\n",
        "\n",
        "**Unigrams:** Cada palabra individual en la lista de tokens (excepto la última, que no se cuenta en este bucle). Se actualizan de manera similar a los bigrams.\n",
        "\n"
      ],
      "metadata": {
        "id": "SNWb_nM6xaEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcule las probabilida de todos los bigramas sin suavizado"
      ],
      "metadata": {
        "id": "_GXbkOVjd2Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilidades de bigramas\n",
        "bigram_prob ={}\n",
        "for bigram, contar in bigram_counts.items():\n",
        "  # Encontrar la frecuencia de la primera palabra del bigrama\n",
        "  prim_palabra= tokens.count(bigram[0])\n",
        "\n",
        "  # Calcular la probabilidad del bigrama\n",
        "  bigram_prob[bigram] = contar/prim_palabra\n",
        "\n",
        "# Resultado\n",
        "for bigram, prob in bigram_prob.items():\n",
        "  print(f\"P({bigram[1]}|{bigram[0]}) = {prob:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSNw217jZgkM",
        "outputId": "08d1f8c8-1bc9-4eb7-c738-377a68503129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(all|<s>) = 1.000\n",
            "P(models|all) = 1.000\n",
            "P(are|models) = 1.000\n",
            "P(wrong|are) = 0.500\n",
            "P(a|wrong) = 0.500\n",
            "P(model|a) = 1.000\n",
            "P(is|model) = 1.000\n",
            "P(wrong|is) = 1.000\n",
            "P(some|wrong) = 0.500\n",
            "P(models|some) = 1.000\n",
            "P(useful|are) = 0.500\n",
            "P(</s>|useful) = 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código calcula las probabilidades de bigramas a partir de las frecuencias obtenidas previamente en la celda anterior.\n",
        "\n",
        "Se crea un diccionario bigram_prob para almacenar las probabilidades condicionales de cada bigrama.\n",
        "\n",
        "El código recorre cada bigrama en bigram_counts (donde se almacenaron las frecuencias de bigramas previamente).\n",
        "\n",
        "Para cada bigrama `(palabra1, palabra2)`, se busca la frecuencia de la primera palabra palabra1 en la lista de tokens `(tokens.count(bigram[0]))`.\n",
        "\n",
        "La salida representa las probabilidades condicionales de que una palabra aparezca después de otra en el corpus dado.\n",
        "\n",
        "**OUTPUT:**\n",
        "\n",
        "Las probabilidades de 1.000 indican que cada vez que se encuentra la primera palabra, siempre le sigue la segunda palabra.\n",
        "\n",
        "Las probabilidades de 0.500 indican que la primera palabra puede ser seguida por dos opciones diferentes con igual probabilidad\n"
      ],
      "metadata": {
        "id": "hDSg-E1Vyt76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcule las probabilidades de todos los bigramas y el bigrama no visto \"a models\" con suavizado add-one"
      ],
      "metadata": {
        "id": "u-9CXcAiZfTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilidades de bigramas con suavizado add-one\n",
        "\n",
        "bigram_prob_addone = {}\n",
        "for bigram in [(w1, w2) for w1 in vocab for w2 in vocab]:\n",
        "  count_bigram = bigram_counts.get(bigram, 0)\n",
        "  count_unigram = unigram_counts.get(bigram[0], 0)\n",
        "\n",
        "  bigram_prob_addone[bigram] = (count_bigram + 1) / (count_unigram + len(vocab))\n",
        "\n",
        "# Resultado\n",
        "for bigram, prob in bigram_prob_addone.items():\n",
        "  print(f\"P({bigram[1]}|{bigram[0]}) = {prob:.3f}\")\n",
        "\n",
        "\n",
        "# Probabilidad del bigrama no visto \"a models\"\n",
        "\n",
        "bigram_no_visto = (\"a\", \"models\")\n",
        "prob_no_visto = bigram_prob_addone.get(bigram_no_visto, 0)\n",
        "print(\"\\nBigrama no visto (a models)\")\n",
        "print(f\"\\nP({bigram_no_visto[1]}|{bigram_no_visto[0]}) = {prob_no_visto:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ93g1ctcO1i",
        "outputId": "fb6ddf68-4e22-43c6-a408-e5f4caaf58cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(model|model) = 0.053\n",
            "P(a|model) = 0.053\n",
            "P(useful|model) = 0.053\n",
            "P(all|model) = 0.053\n",
            "P(are|model) = 0.053\n",
            "P(wrong|model) = 0.053\n",
            "P(some|model) = 0.053\n",
            "P(models|model) = 0.053\n",
            "P(<s>|model) = 0.053\n",
            "P(</s>|model) = 0.053\n",
            "P(model|a) = 0.105\n",
            "P(a|a) = 0.053\n",
            "P(useful|a) = 0.053\n",
            "P(all|a) = 0.053\n",
            "P(are|a) = 0.053\n",
            "P(wrong|a) = 0.053\n",
            "P(some|a) = 0.053\n",
            "P(models|a) = 0.053\n",
            "P(<s>|a) = 0.053\n",
            "P(</s>|a) = 0.053\n",
            "P(model|useful) = 0.053\n",
            "P(a|useful) = 0.053\n",
            "P(useful|useful) = 0.053\n",
            "P(all|useful) = 0.053\n",
            "P(are|useful) = 0.053\n",
            "P(wrong|useful) = 0.053\n",
            "P(some|useful) = 0.053\n",
            "P(models|useful) = 0.053\n",
            "P(<s>|useful) = 0.053\n",
            "P(</s>|useful) = 0.105\n",
            "P(model|all) = 0.053\n",
            "P(a|all) = 0.053\n",
            "P(useful|all) = 0.053\n",
            "P(all|all) = 0.053\n",
            "P(are|all) = 0.053\n",
            "P(wrong|all) = 0.053\n",
            "P(some|all) = 0.053\n",
            "P(models|all) = 0.105\n",
            "P(<s>|all) = 0.053\n",
            "P(</s>|all) = 0.053\n",
            "P(model|are) = 0.036\n",
            "P(a|are) = 0.036\n",
            "P(useful|are) = 0.071\n",
            "P(all|are) = 0.036\n",
            "P(are|are) = 0.036\n",
            "P(wrong|are) = 0.071\n",
            "P(some|are) = 0.036\n",
            "P(models|are) = 0.036\n",
            "P(<s>|are) = 0.036\n",
            "P(</s>|are) = 0.036\n",
            "P(model|wrong) = 0.036\n",
            "P(a|wrong) = 0.071\n",
            "P(useful|wrong) = 0.036\n",
            "P(all|wrong) = 0.036\n",
            "P(are|wrong) = 0.036\n",
            "P(wrong|wrong) = 0.036\n",
            "P(some|wrong) = 0.071\n",
            "P(models|wrong) = 0.036\n",
            "P(<s>|wrong) = 0.036\n",
            "P(</s>|wrong) = 0.036\n",
            "P(model|some) = 0.053\n",
            "P(a|some) = 0.053\n",
            "P(useful|some) = 0.053\n",
            "P(all|some) = 0.053\n",
            "P(are|some) = 0.053\n",
            "P(wrong|some) = 0.053\n",
            "P(some|some) = 0.053\n",
            "P(models|some) = 0.105\n",
            "P(<s>|some) = 0.053\n",
            "P(</s>|some) = 0.053\n",
            "P(model|models) = 0.036\n",
            "P(a|models) = 0.036\n",
            "P(useful|models) = 0.036\n",
            "P(all|models) = 0.036\n",
            "P(are|models) = 0.107\n",
            "P(wrong|models) = 0.036\n",
            "P(some|models) = 0.036\n",
            "P(models|models) = 0.036\n",
            "P(<s>|models) = 0.036\n",
            "P(</s>|models) = 0.036\n",
            "P(model|<s>) = 0.053\n",
            "P(a|<s>) = 0.053\n",
            "P(useful|<s>) = 0.053\n",
            "P(all|<s>) = 0.105\n",
            "P(are|<s>) = 0.053\n",
            "P(wrong|<s>) = 0.053\n",
            "P(some|<s>) = 0.053\n",
            "P(models|<s>) = 0.053\n",
            "P(<s>|<s>) = 0.053\n",
            "P(</s>|<s>) = 0.053\n",
            "P(model|</s>) = 0.091\n",
            "P(a|</s>) = 0.091\n",
            "P(useful|</s>) = 0.091\n",
            "P(all|</s>) = 0.091\n",
            "P(are|</s>) = 0.091\n",
            "P(wrong|</s>) = 0.091\n",
            "P(some|</s>) = 0.091\n",
            "P(models|</s>) = 0.091\n",
            "P(<s>|</s>) = 0.091\n",
            "P(</s>|</s>) = 0.091\n",
            "\n",
            "Bigrama no visto (a models)\n",
            "\n",
            "P(models|a) = 0.053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El suavizado Add-One se utiliza para evitar probabilidades de cero en bigramas no vistos en el corpus. Entonces, si un bigrama no aparece en el conjunto de datos original, se le asigna una probabilidad pequeña en lugar de ser ignorado.\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "Este resultado `P(models|a) = 0.053 ` indica que, según el modelo con suavizado, existe una probabilidad del 5.3% de que la palabra `models` siga a `a`, aunque este bigrama no haya sido visto en el corpus original.\n",
        "\n",
        "Sin el suavizado, la probabilidad de `P(models|a) `sería 0 porque el bigrama `a models` no aparece en el texto. Esto puede ser un problema ya que resultaría en probabilidades nulas para palabras posibles. El suavizado evita este problema y asegura que todos los bigramas tengan al menos una probabilidad pequeña."
      ],
      "metadata": {
        "id": "LjsS6z9qz3KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcule las probabilidades de todos los bigramas y el bigrama no visto \"a models\" con suavizado add-k. Pruebe k=0.05 y k=0.15"
      ],
      "metadata": {
        "id": "hAqFRkxriZpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prob_addk(tokens, vocab, k):\n",
        "\n",
        "  # Probabilidades de bigramas con suavizado add-k\n",
        "  bigram_prob_addk = {}\n",
        "  for bigram in [(w1, w2) for w1 in vocab for w2 in vocab]:\n",
        "    count_bigram = bigram_counts.get(bigram, 0)\n",
        "    count_unigram = unigram_counts.get(bigram[0], 0)\n",
        "    bigram_prob_addk[bigram] = (count_bigram + k) / (count_unigram + k * len(vocab))\n",
        "\n",
        "  return bigram_prob_addk\n",
        "\n",
        "# Calcular probabilidades con k=0.05\n",
        "k005 = prob_addk(tokens, vocab, 0.05)\n",
        "\n",
        "print(\"Probabilidades con suavizado add-k (k=0.05):\\n\")\n",
        "for bigram, prob in k005.items():\n",
        "  print(f\"P({bigram[1]}|{bigram[0]}) = {prob:.3f}\")\n",
        "\n",
        "\n",
        "bigram_no_visto = (\"a\", \"models\")\n",
        "prob005 = k005.get(bigram_no_visto, 0)\n",
        "print(\"\\nBigrama no visto (a models) con k=0.05\")\n",
        "print(f\"\\nP({bigram_no_visto[1]}|{bigram_no_visto[0]}) = {prob005:.3f}\")\n",
        "\n",
        "# Calcular probabilidades con k=0.15\n",
        "k015 = prob_addk(tokens, vocab, 0.15)\n",
        "\n",
        "print(\"\\n\\nProbabilidades con suavizado add-k (k=0.15):\")\n",
        "for bigram, prob in k015.items():\n",
        "  print(f\"P({bigram[1]}|{bigram[0]}) = {prob:.3f}\")\n",
        "\n",
        "bigram_no_visto = (\"a\", \"models\")\n",
        "prob015 = k015.get(bigram_no_visto, 0)\n",
        "print(\"\\nBigrama no visto (a models) con k=0.15\")\n",
        "print(f\"\\nP({bigram_no_visto[1]}|{bigram_no_visto[0]}) = {prob015:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA-mZnvxieBf",
        "outputId": "52605ee0-e29d-474a-fa64-6097708857c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades con suavizado add-k (k=0.05):\n",
            "\n",
            "P(model|model) = 0.005\n",
            "P(a|model) = 0.005\n",
            "P(useful|model) = 0.005\n",
            "P(all|model) = 0.005\n",
            "P(are|model) = 0.005\n",
            "P(wrong|model) = 0.005\n",
            "P(some|model) = 0.005\n",
            "P(models|model) = 0.005\n",
            "P(<s>|model) = 0.005\n",
            "P(</s>|model) = 0.005\n",
            "P(model|a) = 0.111\n",
            "P(a|a) = 0.005\n",
            "P(useful|a) = 0.005\n",
            "P(all|a) = 0.005\n",
            "P(are|a) = 0.005\n",
            "P(wrong|a) = 0.005\n",
            "P(some|a) = 0.005\n",
            "P(models|a) = 0.005\n",
            "P(<s>|a) = 0.005\n",
            "P(</s>|a) = 0.005\n",
            "P(model|useful) = 0.005\n",
            "P(a|useful) = 0.005\n",
            "P(useful|useful) = 0.005\n",
            "P(all|useful) = 0.005\n",
            "P(are|useful) = 0.005\n",
            "P(wrong|useful) = 0.005\n",
            "P(some|useful) = 0.005\n",
            "P(models|useful) = 0.005\n",
            "P(<s>|useful) = 0.005\n",
            "P(</s>|useful) = 0.111\n",
            "P(model|all) = 0.005\n",
            "P(a|all) = 0.005\n",
            "P(useful|all) = 0.005\n",
            "P(all|all) = 0.005\n",
            "P(are|all) = 0.005\n",
            "P(wrong|all) = 0.005\n",
            "P(some|all) = 0.005\n",
            "P(models|all) = 0.111\n",
            "P(<s>|all) = 0.005\n",
            "P(</s>|all) = 0.005\n",
            "P(model|are) = 0.003\n",
            "P(a|are) = 0.003\n",
            "P(useful|are) = 0.057\n",
            "P(all|are) = 0.003\n",
            "P(are|are) = 0.003\n",
            "P(wrong|are) = 0.057\n",
            "P(some|are) = 0.003\n",
            "P(models|are) = 0.003\n",
            "P(<s>|are) = 0.003\n",
            "P(</s>|are) = 0.003\n",
            "P(model|wrong) = 0.003\n",
            "P(a|wrong) = 0.057\n",
            "P(useful|wrong) = 0.003\n",
            "P(all|wrong) = 0.003\n",
            "P(are|wrong) = 0.003\n",
            "P(wrong|wrong) = 0.003\n",
            "P(some|wrong) = 0.057\n",
            "P(models|wrong) = 0.003\n",
            "P(<s>|wrong) = 0.003\n",
            "P(</s>|wrong) = 0.003\n",
            "P(model|some) = 0.005\n",
            "P(a|some) = 0.005\n",
            "P(useful|some) = 0.005\n",
            "P(all|some) = 0.005\n",
            "P(are|some) = 0.005\n",
            "P(wrong|some) = 0.005\n",
            "P(some|some) = 0.005\n",
            "P(models|some) = 0.111\n",
            "P(<s>|some) = 0.005\n",
            "P(</s>|some) = 0.005\n",
            "P(model|models) = 0.003\n",
            "P(a|models) = 0.003\n",
            "P(useful|models) = 0.003\n",
            "P(all|models) = 0.003\n",
            "P(are|models) = 0.111\n",
            "P(wrong|models) = 0.003\n",
            "P(some|models) = 0.003\n",
            "P(models|models) = 0.003\n",
            "P(<s>|models) = 0.003\n",
            "P(</s>|models) = 0.003\n",
            "P(model|<s>) = 0.005\n",
            "P(a|<s>) = 0.005\n",
            "P(useful|<s>) = 0.005\n",
            "P(all|<s>) = 0.111\n",
            "P(are|<s>) = 0.005\n",
            "P(wrong|<s>) = 0.005\n",
            "P(some|<s>) = 0.005\n",
            "P(models|<s>) = 0.005\n",
            "P(<s>|<s>) = 0.005\n",
            "P(</s>|<s>) = 0.005\n",
            "P(model|</s>) = 0.033\n",
            "P(a|</s>) = 0.033\n",
            "P(useful|</s>) = 0.033\n",
            "P(all|</s>) = 0.033\n",
            "P(are|</s>) = 0.033\n",
            "P(wrong|</s>) = 0.033\n",
            "P(some|</s>) = 0.033\n",
            "P(models|</s>) = 0.033\n",
            "P(<s>|</s>) = 0.033\n",
            "P(</s>|</s>) = 0.033\n",
            "\n",
            "Bigrama no visto (a models) con k=0.05\n",
            "\n",
            "P(models|a) = 0.005\n",
            "\n",
            "\n",
            "Probabilidades con suavizado add-k (k=0.15):\n",
            "P(model|model) = 0.014\n",
            "P(a|model) = 0.014\n",
            "P(useful|model) = 0.014\n",
            "P(all|model) = 0.014\n",
            "P(are|model) = 0.014\n",
            "P(wrong|model) = 0.014\n",
            "P(some|model) = 0.014\n",
            "P(models|model) = 0.014\n",
            "P(<s>|model) = 0.014\n",
            "P(</s>|model) = 0.014\n",
            "P(model|a) = 0.110\n",
            "P(a|a) = 0.014\n",
            "P(useful|a) = 0.014\n",
            "P(all|a) = 0.014\n",
            "P(are|a) = 0.014\n",
            "P(wrong|a) = 0.014\n",
            "P(some|a) = 0.014\n",
            "P(models|a) = 0.014\n",
            "P(<s>|a) = 0.014\n",
            "P(</s>|a) = 0.014\n",
            "P(model|useful) = 0.014\n",
            "P(a|useful) = 0.014\n",
            "P(useful|useful) = 0.014\n",
            "P(all|useful) = 0.014\n",
            "P(are|useful) = 0.014\n",
            "P(wrong|useful) = 0.014\n",
            "P(some|useful) = 0.014\n",
            "P(models|useful) = 0.014\n",
            "P(<s>|useful) = 0.014\n",
            "P(</s>|useful) = 0.110\n",
            "P(model|all) = 0.014\n",
            "P(a|all) = 0.014\n",
            "P(useful|all) = 0.014\n",
            "P(all|all) = 0.014\n",
            "P(are|all) = 0.014\n",
            "P(wrong|all) = 0.014\n",
            "P(some|all) = 0.014\n",
            "P(models|all) = 0.110\n",
            "P(<s>|all) = 0.014\n",
            "P(</s>|all) = 0.014\n",
            "P(model|are) = 0.008\n",
            "P(a|are) = 0.008\n",
            "P(useful|are) = 0.059\n",
            "P(all|are) = 0.008\n",
            "P(are|are) = 0.008\n",
            "P(wrong|are) = 0.059\n",
            "P(some|are) = 0.008\n",
            "P(models|are) = 0.008\n",
            "P(<s>|are) = 0.008\n",
            "P(</s>|are) = 0.008\n",
            "P(model|wrong) = 0.008\n",
            "P(a|wrong) = 0.059\n",
            "P(useful|wrong) = 0.008\n",
            "P(all|wrong) = 0.008\n",
            "P(are|wrong) = 0.008\n",
            "P(wrong|wrong) = 0.008\n",
            "P(some|wrong) = 0.059\n",
            "P(models|wrong) = 0.008\n",
            "P(<s>|wrong) = 0.008\n",
            "P(</s>|wrong) = 0.008\n",
            "P(model|some) = 0.014\n",
            "P(a|some) = 0.014\n",
            "P(useful|some) = 0.014\n",
            "P(all|some) = 0.014\n",
            "P(are|some) = 0.014\n",
            "P(wrong|some) = 0.014\n",
            "P(some|some) = 0.014\n",
            "P(models|some) = 0.110\n",
            "P(<s>|some) = 0.014\n",
            "P(</s>|some) = 0.014\n",
            "P(model|models) = 0.008\n",
            "P(a|models) = 0.008\n",
            "P(useful|models) = 0.008\n",
            "P(all|models) = 0.008\n",
            "P(are|models) = 0.110\n",
            "P(wrong|models) = 0.008\n",
            "P(some|models) = 0.008\n",
            "P(models|models) = 0.008\n",
            "P(<s>|models) = 0.008\n",
            "P(</s>|models) = 0.008\n",
            "P(model|<s>) = 0.014\n",
            "P(a|<s>) = 0.014\n",
            "P(useful|<s>) = 0.014\n",
            "P(all|<s>) = 0.110\n",
            "P(are|<s>) = 0.014\n",
            "P(wrong|<s>) = 0.014\n",
            "P(some|<s>) = 0.014\n",
            "P(models|<s>) = 0.014\n",
            "P(<s>|<s>) = 0.014\n",
            "P(</s>|<s>) = 0.014\n",
            "P(model|</s>) = 0.060\n",
            "P(a|</s>) = 0.060\n",
            "P(useful|</s>) = 0.060\n",
            "P(all|</s>) = 0.060\n",
            "P(are|</s>) = 0.060\n",
            "P(wrong|</s>) = 0.060\n",
            "P(some|</s>) = 0.060\n",
            "P(models|</s>) = 0.060\n",
            "P(<s>|</s>) = 0.060\n",
            "P(</s>|</s>) = 0.060\n",
            "\n",
            "Bigrama no visto (a models) con k=0.15\n",
            "\n",
            "P(models|a) = 0.014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El suavizado Add-k modifica el cálculo de probabilidades al sumar un valor `k` a los conteos de cada bigrama. Esto asegura que incluso los bigramas que no se han observado en el corpus reciban una pequeña probabilidad en lugar de cero.\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "`P(models|a) = 0.005, K=0.05`\n",
        "\n",
        "Esta probabilidad indica que, cuando se utiliza un valor pequeño de k=0.05, el modelo asigna una probabilidad muy baja a que la palabra `models` siga a `a`.\n",
        "Dado que el bigrama `a models` no aparece en el corpus original, el suavizado add-k evita que la probabilidad sea cero y asigna un valor positivo pequeño.\n",
        "\n",
        "`P(models|a) = 0.014, K=0.15`\n",
        "\n",
        "Al usar un valor más grande de k=0.15, la probabilidad asignada a `P(models|a)` aumenta a `1.4%`. El incremento en k permite que el modelo dé más peso a los bigramas no observados, elevando sus probabilidades.\n",
        "\n",
        "Podemos concluir que incrementar `k` asigna mayores probabilidades a bigramas no vistos siendo más flexible y menos restringido a los datos originales del corpus.\n"
      ],
      "metadata": {
        "id": "ghe9dZoR7kBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcule las probabilidades de todos los bigramas y el bigrama no visto \"a models\" con back-off y stupid-backoff"
      ],
      "metadata": {
        "id": "mgJeOdU6jZ-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backoff_bigram_probability(w1, w2, unigram_counts, bigram_counts, total_words):\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    if bigram_count > 0:\n",
        "        return bigram_count / unigram_counts.get(w1, 0)\n",
        "    else:\n",
        "        # Back-off to unigram probability\n",
        "        return unigram_counts.get(w2, 0) / total_words\n",
        "\n",
        "# Prob para bigramas\n",
        "for w1 in vocab:\n",
        "    for w2 in vocab:\n",
        "        prob = backoff_bigram_probability(w1, w2, unigram_counts, bigram_counts, total_words)\n",
        "        if prob > 0:\n",
        "            print(f\"P({w2}|{w1}) = {prob:.3f}\")\n",
        "\n",
        "# Prob para \"a models\"\n",
        "probrar_backoff = backoff_bigram_probability('a', 'models', unigram_counts, bigram_counts, total_words)\n",
        "print(f\"P(models|a) con back-off = {probrar_backoff:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyzoQ4B_jjOr",
        "outputId": "0d072b2b-66fb-460e-ab3e-464adb076af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(model|model) = 0.077\n",
            "P(useful|model) = 0.077\n",
            "P(<s>|model) = 0.077\n",
            "P(some|model) = 0.077\n",
            "P(a|model) = 0.077\n",
            "P(wrong|model) = 0.154\n",
            "P(models|model) = 0.154\n",
            "P(are|model) = 0.154\n",
            "P(all|model) = 0.077\n",
            "P(model|useful) = 0.077\n",
            "P(useful|useful) = 0.077\n",
            "P(<s>|useful) = 0.077\n",
            "P(some|useful) = 0.077\n",
            "P(a|useful) = 0.077\n",
            "P(wrong|useful) = 0.154\n",
            "P(models|useful) = 0.154\n",
            "P(are|useful) = 0.154\n",
            "P(all|useful) = 0.077\n",
            "P(</s>|useful) = 1.000\n",
            "P(model|<s>) = 0.077\n",
            "P(useful|<s>) = 0.077\n",
            "P(<s>|<s>) = 0.077\n",
            "P(some|<s>) = 0.077\n",
            "P(a|<s>) = 0.077\n",
            "P(wrong|<s>) = 0.154\n",
            "P(models|<s>) = 0.154\n",
            "P(are|<s>) = 0.154\n",
            "P(all|<s>) = 1.000\n",
            "P(model|some) = 0.077\n",
            "P(useful|some) = 0.077\n",
            "P(<s>|some) = 0.077\n",
            "P(some|some) = 0.077\n",
            "P(a|some) = 0.077\n",
            "P(wrong|some) = 0.154\n",
            "P(models|some) = 1.000\n",
            "P(are|some) = 0.154\n",
            "P(all|some) = 0.077\n",
            "P(model|a) = 1.000\n",
            "P(useful|a) = 0.077\n",
            "P(<s>|a) = 0.077\n",
            "P(some|a) = 0.077\n",
            "P(a|a) = 0.077\n",
            "P(wrong|a) = 0.154\n",
            "P(models|a) = 0.154\n",
            "P(are|a) = 0.154\n",
            "P(all|a) = 0.077\n",
            "P(model|wrong) = 0.077\n",
            "P(useful|wrong) = 0.077\n",
            "P(<s>|wrong) = 0.077\n",
            "P(some|wrong) = 0.500\n",
            "P(a|wrong) = 0.500\n",
            "P(wrong|wrong) = 0.154\n",
            "P(models|wrong) = 0.154\n",
            "P(are|wrong) = 0.154\n",
            "P(all|wrong) = 0.077\n",
            "P(model|models) = 0.077\n",
            "P(useful|models) = 0.077\n",
            "P(<s>|models) = 0.077\n",
            "P(some|models) = 0.077\n",
            "P(a|models) = 0.077\n",
            "P(wrong|models) = 0.154\n",
            "P(models|models) = 0.154\n",
            "P(are|models) = 1.000\n",
            "P(all|models) = 0.077\n",
            "P(model|are) = 0.077\n",
            "P(useful|are) = 0.500\n",
            "P(<s>|are) = 0.077\n",
            "P(some|are) = 0.077\n",
            "P(a|are) = 0.077\n",
            "P(wrong|are) = 0.500\n",
            "P(models|are) = 0.154\n",
            "P(are|are) = 0.154\n",
            "P(all|are) = 0.077\n",
            "P(model|all) = 0.077\n",
            "P(useful|all) = 0.077\n",
            "P(<s>|all) = 0.077\n",
            "P(some|all) = 0.077\n",
            "P(a|all) = 0.077\n",
            "P(wrong|all) = 0.154\n",
            "P(models|all) = 1.000\n",
            "P(are|all) = 0.154\n",
            "P(all|all) = 0.077\n",
            "P(model|</s>) = 0.077\n",
            "P(useful|</s>) = 0.077\n",
            "P(<s>|</s>) = 0.077\n",
            "P(some|</s>) = 0.077\n",
            "P(a|</s>) = 0.077\n",
            "P(wrong|</s>) = 0.154\n",
            "P(models|</s>) = 0.154\n",
            "P(are|</s>) = 0.154\n",
            "P(all|</s>) = 0.077\n",
            "P(models|a) con back-off = 0.154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código implementa un enfoque de back-off para calcular las probabilidades de bigramas, utilizando una estrategia que recurre a las probabilidades de unigramas cuando el bigrama no se ha observado en el corpus.\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "El código genera las probabilidades para todos los posibles bigramas en el vocabulario con el método back-off\n",
        "\n",
        "`P(models|a) con back-off = 0.154`\n",
        "\n",
        "Dado que el bigrama `a models` no se ha observado en el corpus, el cálculo se respalda en la probabilidad del unigrama P(models). La probabilidad resultante de 0.154 muestra que, aunque `models `no aparece directamente después de `a` en el corpus original, la frecuencia de models es suficiente para que el modelo le asigne una probabilidad significativa en combinación con a.\n"
      ],
      "metadata": {
        "id": "ysDsDyoI-2CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stupid backoff\n",
        "def stupid_backoff_bigram_probability(w1, w2, unigram_counts, bigram_counts, total_words, factor_lamda=0.4):\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "    if bigram_count > 0:\n",
        "        return bigram_count / unigram_counts.get(w1, 0)\n",
        "    else:\n",
        "        return factor_lamda * (unigram_counts.get(w2, 0) / total_words)\n",
        "\n",
        "# Calculate probabilidad for all bigrams\n",
        "for w1 in vocab:\n",
        "    for w2 in vocab:\n",
        "        prob = stupid_backoff_bigram_probability(w1, w2, unigram_counts, bigram_counts, total_words)\n",
        "        if prob > 0:\n",
        "            print(f\"P({w2}|{w1}) = {prob:.3f}\")\n",
        "\n",
        "# Calculate probability for unseen bigram \"a models\"\n",
        "probrar_backoff_tonto = stupid_backoff_bigram_probability('a', 'models', unigram_counts, bigram_counts, total_words)\n",
        "print(f\"P(models|a) con stupid back-off = {probrar_backoff_tonto:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c4N7U5qtT8k",
        "outputId": "32463cc7-8cbd-4320-dfc3-6c223b909f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(model|model) = 0.031\n",
            "P(useful|model) = 0.031\n",
            "P(<s>|model) = 0.031\n",
            "P(some|model) = 0.031\n",
            "P(a|model) = 0.031\n",
            "P(wrong|model) = 0.062\n",
            "P(models|model) = 0.062\n",
            "P(are|model) = 0.062\n",
            "P(all|model) = 0.031\n",
            "P(model|useful) = 0.031\n",
            "P(useful|useful) = 0.031\n",
            "P(<s>|useful) = 0.031\n",
            "P(some|useful) = 0.031\n",
            "P(a|useful) = 0.031\n",
            "P(wrong|useful) = 0.062\n",
            "P(models|useful) = 0.062\n",
            "P(are|useful) = 0.062\n",
            "P(all|useful) = 0.031\n",
            "P(</s>|useful) = 1.000\n",
            "P(model|<s>) = 0.031\n",
            "P(useful|<s>) = 0.031\n",
            "P(<s>|<s>) = 0.031\n",
            "P(some|<s>) = 0.031\n",
            "P(a|<s>) = 0.031\n",
            "P(wrong|<s>) = 0.062\n",
            "P(models|<s>) = 0.062\n",
            "P(are|<s>) = 0.062\n",
            "P(all|<s>) = 1.000\n",
            "P(model|some) = 0.031\n",
            "P(useful|some) = 0.031\n",
            "P(<s>|some) = 0.031\n",
            "P(some|some) = 0.031\n",
            "P(a|some) = 0.031\n",
            "P(wrong|some) = 0.062\n",
            "P(models|some) = 1.000\n",
            "P(are|some) = 0.062\n",
            "P(all|some) = 0.031\n",
            "P(model|a) = 1.000\n",
            "P(useful|a) = 0.031\n",
            "P(<s>|a) = 0.031\n",
            "P(some|a) = 0.031\n",
            "P(a|a) = 0.031\n",
            "P(wrong|a) = 0.062\n",
            "P(models|a) = 0.062\n",
            "P(are|a) = 0.062\n",
            "P(all|a) = 0.031\n",
            "P(model|wrong) = 0.031\n",
            "P(useful|wrong) = 0.031\n",
            "P(<s>|wrong) = 0.031\n",
            "P(some|wrong) = 0.500\n",
            "P(a|wrong) = 0.500\n",
            "P(wrong|wrong) = 0.062\n",
            "P(models|wrong) = 0.062\n",
            "P(are|wrong) = 0.062\n",
            "P(all|wrong) = 0.031\n",
            "P(model|models) = 0.031\n",
            "P(useful|models) = 0.031\n",
            "P(<s>|models) = 0.031\n",
            "P(some|models) = 0.031\n",
            "P(a|models) = 0.031\n",
            "P(wrong|models) = 0.062\n",
            "P(models|models) = 0.062\n",
            "P(are|models) = 1.000\n",
            "P(all|models) = 0.031\n",
            "P(model|are) = 0.031\n",
            "P(useful|are) = 0.500\n",
            "P(<s>|are) = 0.031\n",
            "P(some|are) = 0.031\n",
            "P(a|are) = 0.031\n",
            "P(wrong|are) = 0.500\n",
            "P(models|are) = 0.062\n",
            "P(are|are) = 0.062\n",
            "P(all|are) = 0.031\n",
            "P(model|all) = 0.031\n",
            "P(useful|all) = 0.031\n",
            "P(<s>|all) = 0.031\n",
            "P(some|all) = 0.031\n",
            "P(a|all) = 0.031\n",
            "P(wrong|all) = 0.062\n",
            "P(models|all) = 1.000\n",
            "P(are|all) = 0.062\n",
            "P(all|all) = 0.031\n",
            "P(model|</s>) = 0.031\n",
            "P(useful|</s>) = 0.031\n",
            "P(<s>|</s>) = 0.031\n",
            "P(some|</s>) = 0.031\n",
            "P(a|</s>) = 0.031\n",
            "P(wrong|</s>) = 0.062\n",
            "P(models|</s>) = 0.062\n",
            "P(are|</s>) = 0.062\n",
            "P(all|</s>) = 0.031\n",
            "P(models|a) con stupid back-off = 0.062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código utiliza stupid backoff, que es una variante simplificada del backoff tradicional en modelos de lenguaje.\n",
        "\n",
        "**OTUPUT:**\n",
        "\n",
        "`P(models|a) con stupid back-off = 0.062`\n",
        "\n",
        "El resultado 0.062 indica que la probabilidad del bigrama no visto `a models `es del 6.2%. Esta probabilidad es calculada multiplicando la probabilidad del unigrama `P(models)` por el factor` λ (0.4)`. Esto refleja la confianza reducida que tiene el modelo en la combinación no vista en comparación con un bigrama observado.\n",
        "\n"
      ],
      "metadata": {
        "id": "F4nDEYnbAyuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones parte 1"
      ],
      "metadata": {
        "id": "yTLANFMHBb20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variación en las probabilidades `P(models|a)` se debe a cómo cada método maneja la parte de bigramas no vistos. **Add-one y add-k** suavizan asignando pequeñas probabilidades uniformementen, mientras que, **Back-off y stupid backoff** usan la frecuencia general de models, dando más flexibilidad y ajustando las probabilidades según la proporcion de unigramas. Esto causa que métodos como back-off asignen probabilidades más altas comparado con los suavizados, que distribuyen el peso de forma más pareja."
      ],
      "metadata": {
        "id": "PPFNNCCxBflF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2"
      ],
      "metadata": {
        "id": "QUmy0tn2YfnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTA: ES NECESARIO RECORRER ESTA CELDA PARA CONTINUAR CON LAS DEMAS EN LA PARTE 2\n",
        "\n",
        "# Suavizado Good-Turing: https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase2/Modelos-lenguaje2.ipynb\n",
        "# Maxima verosimilitud https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase2/Modelos-lenguaje2.ipynb\n",
        "\n",
        "corpus = \"all models are wrong. a model is wrong. some models are useful\"\n",
        "vocab = {'<s>','</s>','<UNK>','a', 'all','are','model','models','some','useful','wrong'}\n",
        "\n",
        "# Tokenizar\n",
        "tokens = corpus.lower().replace('.', '').split()\n",
        "tokens.insert(0, '<s>')\n",
        "tokens.append('</s>')\n",
        "\n",
        "# Reemplazamos palabras desconocidas por <UNK>\n",
        "tokens = [token if token in vocab else '<UNK>' for token in tokens]\n",
        "\n"
      ],
      "metadata": {
        "id": "jPVxe5kDukw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reemplazo de Palabras Desconocidas (`<UNK>`):**\n",
        "\n",
        "Cada token se verifica en el vocabulario, si una palabra no está en el conjunto vocab, se reemplaza por `<UNK>`."
      ],
      "metadata": {
        "id": "PAaimxEPFpcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Good Turing"
      ],
      "metadata": {
        "id": "I_USggOsui9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Cálculo de unigramas\n",
        "unigram_counts = Counter(tokens)\n",
        "\n",
        "# Calculamos r (frecuencia de unigramas) y N_r (número de unigramas con frecuencia r)\n",
        "count_of_counts = Counter(unigram_counts.values())\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(\"Conteos de unigramas (r):\", unigram_counts)\n",
        "print(\"Conteo de frecuencias N_r:\", count_of_counts)\n",
        "\n",
        "# Good-Turing\n",
        "def good_turing_counts(unigram_counts):\n",
        "    count_of_counts = Counter(unigram_counts.values())\n",
        "    adjusted_counts = {}\n",
        "\n",
        "    for unigram, count in unigram_counts.items():\n",
        "        if count + 1 in count_of_counts:\n",
        "            adjusted_counts[unigram] = (count + 1) * (count_of_counts[count + 1] / count_of_counts[count])\n",
        "        else:\n",
        "            adjusted_counts[unigram] = count\n",
        "\n",
        "    return adjusted_counts\n",
        "\n",
        "# Aplicamos Good-Turing a los unigramas\n",
        "adjusted_unigram_counts = good_turing_counts(unigram_counts)\n",
        "\n",
        "# Mostramos los resultados ajustados\n",
        "print(\"Conteos ajustados con Good-Turing:\", adjusted_unigram_counts)\n"
      ],
      "metadata": {
        "id": "7Jsbs9GIuiUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f4b599-7851-423c-914c-1e356095c385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conteos de unigramas (r): Counter({'models': 2, 'are': 2, 'wrong': 2, '<s>': 1, 'all': 1, 'a': 1, 'model': 1, '<UNK>': 1, 'some': 1, 'useful': 1, '</s>': 1})\n",
            "Conteo de frecuencias N_r: Counter({1: 8, 2: 3})\n",
            "Conteos ajustados con Good-Turing: {'<s>': 0.75, 'all': 0.75, 'models': 2, 'are': 2, 'wrong': 2, 'a': 0.75, 'model': 0.75, '<UNK>': 0.75, 'some': 0.75, 'useful': 0.75, '</s>': 0.75}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El método Good Turing primero, calcula cuántas veces aparece cada palabra (unigrama) en el corpus. Luego, determina cuántos unigramas tienen exactamente la misma frecuencia, agrupando los conteos para ver cuántas palabras aparecen solo una vez o dos veces.\n",
        "\n",
        "Para ajustar los conteos, el método analiza las palabras que ocurren solo una vez y reduce su conteo original, asegurando que las probabilidades reflejen la posibilidad de que haya palabras que no se hayan visto en el corpus. Si una palabra tiene una frecuencia baja (por ejemplo, aparece solo una vez), el método ajusta ese valor hacia abajo, mientras que mantiene los conteos originales para palabras más comunes. De esta forma, Good-Turing reasigna parte del conteo de las palabras observadas a posibles palabras que podrían no haber aparecido, mejorando la precisión del modelo en casos de datos escasos.\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "`Counter({'models': 2, 'are': 2, 'wrong': 2, '<s>': 1, 'all': 1, 'a': 1, 'model': 1, '<UNK>': 1, 'some': 1, 'useful': 1, '</s>': 1})`\n",
        "\n",
        "Esto muestra que models, are, y wrong aparecen 2 veces cada uno, mientras que todos los demás unigramas, incluidos los tokens <s> y </s>, aparecen solo una vez.\n",
        "\n",
        "`Counter({1: 8, 2: 3})`\n",
        "\n",
        "Hay 8 unigramas que aparecen solo una vez y 3 unigramas que aparecen dos veces en el corpus. Esto indica que la mayoría de los unigramas son raros.\n",
        "\n",
        "`{'<s>': 0.75, 'all': 0.75, 'models': 2, 'are': 2, 'wrong': 2, 'a': 0.75, 'model': 0.75, '<UNK>': 0.75, 'some': 0.75, 'useful': 0.75, '</s>': 0.75}`\n",
        "\n",
        "Los unigramas quetenían un conteo de 1 se ajustan a 0.75. Esto se debe a que el método Good-Turing redistribuye el conteo para reflejar la posibilidad de eventos no observados reduciendo ligeramente los conteos de palabras raras.\n",
        "\n",
        "Los unigramas que ya tenían un conteo de 2 se mantienen sin cambios porque el ajuste no se aplica en casos donde hay suficiente frecuencia.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P1YTw7i7Emye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (f) **Para** r < 3 calcular las probabilidades de todos los unigramas\n",
        "\n"
      ],
      "metadata": {
        "id": "DTDDIkGxvZ_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Good-Turing\n",
        "def good_turing_counts(ngram_counts):\n",
        "    count_of_counts = Counter(ngram_counts.values())\n",
        "    adjusted_counts = {}\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if count + 1 in count_of_counts:\n",
        "            adjusted_counts[ngram] = (count + 1) * (count_of_counts[count + 1] / count_of_counts[count])\n",
        "        else:\n",
        "            adjusted_counts[ngram] = count\n",
        "\n",
        "    return adjusted_counts\n",
        "\n",
        "# Cálculo de unigramas\n",
        "unigram_counts = Counter(tokens)\n",
        "\n",
        "# Aplicar suavizado Good-Turing\n",
        "adjusted_unigram_counts = good_turing_counts(unigram_counts)\n",
        "\n",
        "# Calcular probabilidades para r < 3\n",
        "total_tokens = sum(unigram_counts.values())\n",
        "probabilidad = {}\n",
        "\n",
        "for unigram, adjusted_count in adjusted_unigram_counts.items():\n",
        "    if unigram_counts[unigram] < 3:  # r < 3\n",
        "        probabilidad[unigram] = adjusted_count / total_tokens\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Probabilidades para unigramas con r < 3:\")\n",
        "for unigram, prob in probabilidad.items():\n",
        "    print(f\"Unigrama: {unigram}, Probabilidad: {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "SLemesm7vlS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6279cd43-cff3-4cd1-eb7c-eb95bf394b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades para unigramas con r < 3:\n",
            "Unigrama: <s>, Probabilidad: 0.0536\n",
            "Unigrama: all, Probabilidad: 0.0536\n",
            "Unigrama: models, Probabilidad: 0.1429\n",
            "Unigrama: are, Probabilidad: 0.1429\n",
            "Unigrama: wrong, Probabilidad: 0.1429\n",
            "Unigrama: a, Probabilidad: 0.0536\n",
            "Unigrama: model, Probabilidad: 0.0536\n",
            "Unigrama: <UNK>, Probabilidad: 0.0536\n",
            "Unigrama: some, Probabilidad: 0.0536\n",
            "Unigrama: useful, Probabilidad: 0.0536\n",
            "Unigrama: </s>, Probabilidad: 0.0536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código filtra los unigramas que tienen una frecuencia (r) menor a 3. Solo se calculan las probabilidades para estos unigramas.\n",
        "La probabilidad de cada unigrama se obtiene dividiendo su conteo ajustado por el total de tokens en el corpus. Esto asegura que las probabilidades estén normalizadas y sumen 1.\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "La mayoría de los unigramas aparecen solo una vez (r = 1) y sus probabilidades ajustadas son alrededor de 0.0536. Esto muestra cómo el método Good-Turing reduce  sus conteos reales para tener algo de probabilidad para posibles eventos no observados.\n",
        "\n",
        "Las palabras que aparecen más de una vez, como `models`, `are` y `wrong`, tienen probabilidades más altas (0.1429). Esto indica que Good-Turing mantiene sus conteos ajustados cercanos a los valores originales, ya que hay suficiente información para estimar su frecuencia con más precisión."
      ],
      "metadata": {
        "id": "gCIDqb0lJpQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  (g) Calcular probabilidades de unigramas de la parte 1 que aparecen con mayor frecuencia $ r= 3$"
      ],
      "metadata": {
        "id": "EhJjN0SPvnEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Cálculo de unigramas\n",
        "unigram_counts = Counter(tokens)\n",
        "\n",
        "# Total de tokens en el corpus\n",
        "total_tokens = sum(unigram_counts.values())\n",
        "\n",
        "# Determinar el valor máximo de r (frecuencia)\n",
        "max_r = max(unigram_counts.values())\n",
        "\n",
        "# Probabilidad con MLE para unigramas con r = max_r (r = 2) ya que no hay palabras que se repitan 3 veces en este corpus\n",
        "probabilidad_mle = {}\n",
        "\n",
        "for unigram, count in unigram_counts.items():\n",
        "    if count == max_r:  # Solo para los unigramas con la frecuencia máxima\n",
        "        probabilidad_mle[unigram] = count / total_tokens\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"Probabilidad estimada por MLE para unigramas con r = {max_r}:\")\n",
        "for unigram, prob in probabilidad_mle.items():\n",
        "    print(f\"Unigrama: {unigram}, Probabilidad: {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "vyB2bDSgv1AS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd348af9-d171-44f0-c7d1-acfaab43843b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidad estimada por MLE para unigramas con r = 2:\n",
            "Unigrama: models, Probabilidad: 0.1429\n",
            "Unigrama: are, Probabilidad: 0.1429\n",
            "Unigrama: wrong, Probabilidad: 0.1429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se identifica la frecuencia máxima `r` en el corpus. En este caso, el valor\n",
        "máximo de `r` es 2, ya que no hay palabras que se repitan 3 veces o más. Para los unigramas que tienen la frecuencia máxima (r = 2), la probabilidad se calcula dividiendo el conteo de cada unigrama entre el total de tokens en el corpus.\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "Las palabras `models`, `are` y `wrong` tienen la frecuencia máxima `r=2` en el corpus. Cada una aparece 2 veces. Usando MLE, la probabilidad de cada una de estas palabras es `0.1429`, lo que se calcula dividiendo 2 (frecuencia de la palabra) entre 14 (total de tokens). Estas probabilidades indican que hay un `14.29%` de posibilidad de que cualquiera de estas palabras aparezca en una posición dada."
      ],
      "metadata": {
        "id": "ucTKCbcbNus3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demostrar que la suma de prob en (f) y (g) no es 1. Normaliza las probabilidades"
      ],
      "metadata": {
        "id": "jEfF5KI_v1f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Good-Turing\n",
        "def good_turing_counts(ngram_counts):\n",
        "    count_of_counts = Counter(ngram_counts.values())\n",
        "    adjusted_counts = {}\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if count + 1 in count_of_counts:\n",
        "            adjusted_counts[ngram] = (count + 1) * (count_of_counts[count + 1] / count_of_counts[count])\n",
        "        else:\n",
        "            adjusted_counts[ngram] = count\n",
        "\n",
        "    return adjusted_counts\n",
        "\n",
        "# Cálculo de unigramas\n",
        "unigram_counts = Counter(tokens)\n",
        "\n",
        "# Total de tokens en el corpus\n",
        "total_tokens = sum(unigram_counts.values())\n",
        "\n",
        "# Aplicar suavizado Good-Turing para r < 3\n",
        "adjusted_unigram_counts = good_turing_counts(unigram_counts)\n",
        "\n",
        "# Calcular las probabilidades con Good-Turing para r < 3\n",
        "probabilidad_f = {}\n",
        "for unigram, adjusted_count in adjusted_unigram_counts.items():\n",
        "    if unigram_counts[unigram] < 3:  # r < 3\n",
        "        probabilidad_f[unigram] = adjusted_count / total_tokens\n",
        "\n",
        "# Probabilidades con MLE para unigramas con r = 2\n",
        "max_r = 2\n",
        "probabilidad_g = {}\n",
        "for unigram, count in unigram_counts.items():\n",
        "    if count == max_r:  # Solo para los unigramas con la frecuencia máxima\n",
        "        probabilidad_g[unigram] = count / total_tokens\n",
        "\n",
        "# Sumar las probabilidades de (f) y (g)\n",
        "prob_total = sum(probabilidad_f.values()) + sum(probabilidad_g.values())\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Probabilidades en (f):\", probabilidad_f)\n",
        "print(\"Probabilidades en (g):\", probabilidad_g)\n",
        "print(f\"Suma total de las probabilidades en (f) y (g): {prob_total:.4f}\")\n",
        "\n",
        "# Normalizar las probabilidades si la suma no es 1\n",
        "if prob_total != 1.0:\n",
        "    normalizacion = 1 / prob_total\n",
        "    probabilidad_f = {unigram: prob * normalizacion for unigram, prob in probabilidad_f.items()}\n",
        "    probabilidad_g = {unigram: prob * normalizacion for unigram, prob in probabilidad_g.items()}\n",
        "    prob_normalizada = sum(probabilidad_f.values()) + sum(probabilidad_g.values())\n",
        "    print(f\"Suma total normalizada de las probabilidades: {prob_normalizada:.4f}\")\n"
      ],
      "metadata": {
        "id": "XAi2H3BQwjNV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987bccec-3d0a-4c50-a362-2e8ea6211514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades en (f): {'<s>': 0.05357142857142857, 'all': 0.05357142857142857, 'models': 0.14285714285714285, 'are': 0.14285714285714285, 'wrong': 0.14285714285714285, 'a': 0.05357142857142857, 'model': 0.05357142857142857, '<UNK>': 0.05357142857142857, 'some': 0.05357142857142857, 'useful': 0.05357142857142857, '</s>': 0.05357142857142857}\n",
            "Probabilidades en (g): {'models': 0.14285714285714285, 'are': 0.14285714285714285, 'wrong': 0.14285714285714285}\n",
            "Suma total de las probabilidades en (f) y (g): 1.2857\n",
            "Suma total normalizada de las probabilidades: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código suma las probabilidades calculadas en (f) y (g). En este caso, la suma total es 1.2857, lo que indica que las probabilidades no están normalizadas (la suma debe ser exactamente 1 para ser válidas en un modelo de probabilidad).\n",
        "\n",
        "Para corregir esto, se normalizan las probabilidades dividiendo cada una por el total (1.2857) para que la suma sea 1. Este proceso asegura que el modelo sea consistente y cumpla con las propiedades de las distribuciones de probabilidad.\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "La variación se da porque Good-Turing redistribuye probabilidades para eventos raros, mientras que MLE asigna directamente a eventos frecuentes sin ajustes. Cuando se suman, las probabilidades exceden en 1, lo que requiere normalización para asegurar que el total unidad."
      ],
      "metadata": {
        "id": "E6a_cqzPP9qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## USAR N5 y el valor original de los recuentos de frecuencia para calcular las probabilidades de todas las unigramas"
      ],
      "metadata": {
        "id": "9Bx1IveNwHpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Good-Turing con N5\n",
        "def good_turing_counts(ngram_counts, adjusted_N5):\n",
        "    count_of_counts = Counter(ngram_counts.values())\n",
        "\n",
        "    count_of_counts[5] = adjusted_N5\n",
        "\n",
        "    adjusted_counts = {}\n",
        "\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        if count + 1 in count_of_counts:\n",
        "            adjusted_counts[ngram] = (count + 1) * (count_of_counts[count + 1] / count_of_counts[count])\n",
        "        else:\n",
        "            adjusted_counts[ngram] = count\n",
        "\n",
        "    return adjusted_counts\n",
        "\n",
        "# Corpus actualizado con la nueva oración\n",
        "corpus = \"all models are wrong. a model is wrong wrong wrong wrong. some models are useful\"\n",
        "vocab = {'<s>', '</s>', '<UNK>', 'a', 'all', 'are', 'model', 'models', 'some', 'useful', 'wrong'}\n",
        "\n",
        "# Tokenización\n",
        "tokens = corpus.lower().replace('.', '').split()\n",
        "tokens.insert(0, '<s>')\n",
        "tokens.append('</s>')\n",
        "\n",
        "# Reemplazamos las palabras fuera del vocabulario por <UNK>\n",
        "tokens = [token if token in vocab else '<UNK>' for token in tokens]\n",
        "\n",
        "# Cálculo de unigramas\n",
        "unigram_counts = Counter(tokens)\n",
        "\n",
        "# Valor ajustado de N_5\n",
        "# Utilizamos una aproximación lineal sencilla, un valor razonable podría ser cercano a 0.5\n",
        "adjusted_N5 = 0.5\n",
        "\n",
        "# Aplicamos Good-Turing con el valor ajustado de N_5\n",
        "adjusted_unigram_counts = good_turing_counts(unigram_counts, adjusted_N5)\n",
        "\n",
        "# Total de tokens en el corpus\n",
        "total_tokens = sum(unigram_counts.values())\n",
        "\n",
        "# Calcular las probabilidades usando los conteos ajustados\n",
        "probabilidades= {unigram: adjusted_count / total_tokens for unigram, adjusted_count in adjusted_unigram_counts.items()}\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Probabilidades con N_5 suavizado:\")\n",
        "for unigram, prob in probabilidades.items():\n",
        "    print(f\"Unigrama: {unigram}, Probabilidad: {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "D09nN1XEwjsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b025251-df2c-4dc4-b4b0-72e71a0c1145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades con N_5 suavizado:\n",
            "Unigrama: <s>, Probabilidad: 0.0294\n",
            "Unigrama: all, Probabilidad: 0.0294\n",
            "Unigrama: models, Probabilidad: 0.1176\n",
            "Unigrama: are, Probabilidad: 0.1176\n",
            "Unigrama: wrong, Probabilidad: 0.2941\n",
            "Unigrama: a, Probabilidad: 0.0294\n",
            "Unigrama: model, Probabilidad: 0.0294\n",
            "Unigrama: <UNK>, Probabilidad: 0.0294\n",
            "Unigrama: some, Probabilidad: 0.0294\n",
            "Unigrama: useful, Probabilidad: 0.0294\n",
            "Unigrama: </s>, Probabilidad: 0.0294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código usa una aproximación lineal para suavizar N5, Aquí se utiliza un valor de 0.5, evitando que N5 sea cero y se actualizó el corpus para que la palabra wrong aparezca 4 veces en una frase, asegurando que `N4 = 1`\n",
        "\n",
        "**OUTPUT**\n",
        "\n",
        "La palabra `wrong` tiene una probabilidad de `0.2941` porque aparece 4 veces en el corpus, lo que indica que es la palabra más frecuente.\n",
        "Este ajuste se ve influenciado por el uso de Good-Turing y la consideración del valor suavizado de N5. Al asignar un valor aproximado de 0.5 a N5 , el modelo evita dar probabilidades de cero a eventos casi nulos, como eventos que podrían aparecer 5 veces. Esto permite al modelo manejar la falta de datos"
      ],
      "metadata": {
        "id": "X_sF6MbYT7Ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 3"
      ],
      "metadata": {
        "id": "W7fv1HVkYhWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Corpus**\n",
        "\n",
        "Para este ejercicio, hemos utilizado un corpus de texto en español basado en un archivo llamado infopankki.en-es.es. Este archivo tiene como contenido la pagina de tour de Finlandia\n",
        "\n",
        "**Descarga y Extracción**\n",
        "\n",
        "Para comenzar, se carga el archivo infopankki.en-es.es. Este archivo es leído línea por línea utilizando la función leer_archivo. Posteriormente, se toma una muestra aleatoria de 1000 líneas para crear un conjunto de datos manejable.\n",
        "\n",
        "**Tokenización**\n",
        "\n",
        "El código convierte cada línea de texto a minúsculas y reemplaza cualquier puntuación con espacios. Esto se realiza mediante la función tokenizar, que también divide el texto en palabras individuales (tokens). Se maneja la puntuación y la conversión entre mayúsculas y minúsculas para estandarizar el texto.\n",
        "\n",
        "**Lematización/Stemming**\n",
        "\n",
        "La función lematizar se encarga de simplificar las palabras a su forma base. En este código, se eliminan los sufijos  como \"s\" para derivar una forma raíz. Ayuda a reducir la variabilidad en el texto al consolidar diferentes formas de una misma palabra.\n",
        "\n",
        "**Filtrado de Palabras Raras**\n",
        "\n",
        "Para reducir el tamaño del vocabulario y eliminar el ruido, el código filtra las palabras que aparecen menos de 5 veces en el corpus. Esto asegura que solo se mantengan las palabras más relevantes y frecuentes.\n",
        "\n",
        "Construcción del Vocabulario\n",
        "\n",
        "Después del preprocesamiento, se utiliza la función `construir_vocabulario` para generar un diccionario que asigna un identificador único a cada palabra en el corpus. Esto crea un vocabulario que se utilizará para entrenar modelos posteriores."
      ],
      "metadata": {
        "id": "uWewQPLbxGF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenido de https://github.com/Kinartb/CC0C2/blob/main/PC1/pc1_nlp.py\n",
        "# Word2vec y GloVe:  https://github.com/kapumota/Actividades-CC0C2/blob/main/Cuadernos-CC0C2/Clase4/Embeddings.ipynb\n",
        "import math\n",
        "import collections\n",
        "from typing import List, Tuple, Dict\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "#########\n",
        "# Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "#########\n",
        "# Seleccion y prepacion del corpus\n",
        "\n",
        "def leer_archivo(ruta):\n",
        "    with open(ruta, 'r', encoding='utf-8') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "# Usamos el archivo infopankki.en-es.es\n",
        "\n",
        "ruta = 'infopankki.en-es.es'\n",
        "archivo = leer_archivo(ruta)\n",
        "\n",
        "# Se toma una mustra aleatorio de 1000 lineas del corpus original\n",
        "\n",
        "\n",
        "def preprocesar_archivo(archivo, num_muestras=1000, umbral_frecuencia=5):\n",
        "    documentos = []\n",
        "    # Distribucion de frecuencia del corpus\n",
        "    todas_palabras = []\n",
        "    for muestra in random.sample(archivo, num_muestras):\n",
        "        tokens = tokenizar(muestra)\n",
        "        todas_palabras.extend(tokens)\n",
        "\n",
        "    distribucion_frecuencia = nltk.FreqDist(todas_palabras)\n",
        "\n",
        "    for muestra in random.sample(archivo, num_muestras):\n",
        "        # Tokenizacion\n",
        "        tokens = tokenizar(muestra)\n",
        "        # Lemantizacion/Stemming\n",
        "        tokens = [Lematizar(token) for token in tokens]\n",
        "        # Remocion stopwords\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "        # Filtrado de palabras raras\n",
        "        tokens = [token for token in tokens if distribucion_frecuencia[token] >= umbral_frecuencia]\n",
        "        documentos.append(tokens)\n",
        "    return documentos\n",
        "\n",
        "############\n",
        "# Tokenizacion\n",
        "\n",
        "def tokenizar(texto):\n",
        "  # Convertir a minúsculas.\n",
        "  texto = texto.lower()\n",
        "  # Reemplazar la puntuación con espacios\n",
        "  texto = re.sub(r'[^\\w\\s]', ' ', texto)\n",
        "  # Dividir el texto en palabras\n",
        "  tokens = texto.split()\n",
        "  return tokens\n",
        "#############\n",
        "# Lemantizacion/Stemming\n",
        "def Lematizar(token):\n",
        "    if token.endswith('s'):\n",
        "        return token[:-1]\n",
        "    return token\n",
        "##############\n",
        "# Fin de preprocesamiento\n",
        "\n",
        "#############\n",
        "\n",
        "##############\n",
        "corpus_entrenamiento = preprocesar_archivo(archivo)\n",
        "print(f\"Tamaño del corpus de entrenamiento: {len(corpus_entrenamiento)}\")\n",
        "\n",
        "def construir_vocabulario(corpus: List[List[str]]) -> Dict[str, int]:\n",
        "    vocabulario = {}\n",
        "    for documento in corpus:\n",
        "        for token in documento:\n",
        "            if token not in vocabulario:\n",
        "                vocabulario[token] = len(vocabulario)\n",
        "    return vocabulario\n",
        "\n",
        "vocabulario = construir_vocabulario(corpus_entrenamiento)\n",
        "tam_vocabulario = len(vocabulario)\n",
        "print(f\"Tamaño del vocabulario: {tam_vocabulario}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf-QwcO98eKa",
        "outputId": "097078e9-a6d3-4f39-f04a-d832ac1c3881"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del corpus de entrenamiento: 1000\n",
            "Tamaño del vocabulario: 220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al finalizar el proceso, se obtiene el tamaño del corpus y el vocabulario generado donde:\n",
        "\n",
        "Tamaño del corpus de entrenamiento: Muestra la cantidad de documentos (líneas de texto tokenizadas) procesadas.\n",
        "\n",
        "Tamaño del vocabulario: Cuántas palabras únicas quedaron después del filtrado."
      ],
      "metadata": {
        "id": "iAmBQL8N_GWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brown Clustering"
      ],
      "metadata": {
        "id": "-z3tMAvP3CIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brown Clustering simple"
      ],
      "metadata": {
        "id": "auRFv7-hKIWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inicialización**\n",
        "\n",
        "Se asigna cada palabra a su propio cluster inicial utilizando la función `inicializar_clusters`. Esto crea un diccionario donde cada palabra se asigna a un ID único de cluster, permitiendo que cada palabra comience como su propio cluster independiente.\n",
        "\n",
        "**Cálculo de Probabilidades**\n",
        "\n",
        "Para estimar las probabilidades de los clusters se calcula` P(c)` y `P(c_i,c_j)` que representa la probabilidad de cada cluster basado en la frecuencia de aparición de sus palabras en el corpus procesado. Se realiza en la función `calcular_probabilidades_Pc`, que determina qué tan común es cada cluster.\n",
        "\n",
        "**Búsqueda de Fusión Óptima**\n",
        "\n",
        "Se selecciona el par de clusters con el mínimo `ΔI` calculado y se realiza su fusión. Después de cada fusión, se actualizan las probabilidades `P(c)` para reflejar los cambios en los clusters. Este proceso se repite hasta alcanzar el número de cluester.\n",
        "\n",
        "\n",
        "**Fusión de Clusters**\n",
        "\n",
        "Se selecciona el par de clusters con el mínimo ΔI calculado y se realiza su fusión. Después de cada fusión, se actualizan las probabilidades `P(c)` para reflejar los cambios en los clusters. Este proceso se repite hasta alcanzar el número deseado de clusters, en este caso 100.\n",
        "\n",
        "**Repetición**\n",
        "\n",
        "El proceso de fusión continúa hasta que se alcanzan los `100` clusters deseados. Cada vez que se realiza una fusión, el código imprime el número de clusters restantes."
      ],
      "metadata": {
        "id": "2TGG4DdaKsl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "##########\n",
        "# Inicialización (Asignar cada palabra a su propio cluster)\n",
        "\n",
        "def inicializar_clusters(vocabulario: Dict[str, int]) -> Dict[int, List[str]]:\n",
        "    clusters = {i: [palabra] for palabra, i in vocabulario.items()}\n",
        "    return clusters\n",
        "\n",
        "clusters = inicializar_clusters(vocabulario)\n",
        "print(f\"Número inicial de clusters: {len(clusters)}\")\n",
        "\n",
        "##########\n",
        "# Cálculo de probabilidades P(c)\n",
        "\n",
        "def calcular_probabilidades_Pc(clusters: Dict[int, List[str]], corpus: List[List[str]]) -> Dict[int, float]:\n",
        "    total_palabras = sum(len(doc) for doc in corpus)\n",
        "    P_c = {}\n",
        "\n",
        "    # Contar frecuencia de clases\n",
        "    for c_id, palabras in clusters.items():\n",
        "        frecuencia_cluster = sum(1 for doc in corpus for token in doc if token in palabras)\n",
        "        P_c[c_id] = frecuencia_cluster / total_palabras\n",
        "\n",
        "    return P_c\n",
        "\n",
        "P_c = calcular_probabilidades_Pc(clusters, corpus_entrenamiento)\n",
        "print(\"Probabilidades iniciales calculadas.\")\n",
        "\n",
        "##########\n",
        "# Fusión de clusters\n",
        "\n",
        "def fusionar_clusters_simple(clusters: Dict[int, List[str]], P_c: Dict[int, float], corpus: List[List[str]], num_clusters_deseado: int, epsilon: float = 1e-10):\n",
        "    total_palabras = sum(len(doc) for doc in corpus)\n",
        "\n",
        "    while len(clusters) > num_clusters_deseado:\n",
        "        min_delta_I = None\n",
        "        pair_to_merge = None\n",
        "\n",
        "        cluster_ids = list(clusters.keys())\n",
        "        for idx_i in range(len(cluster_ids)):\n",
        "            c_i = cluster_ids[idx_i]\n",
        "            palabras_i = set(clusters[c_i])\n",
        "            for idx_j in range(idx_i + 1, len(cluster_ids)):\n",
        "                c_j = cluster_ids[idx_j]\n",
        "                palabras_j = set(clusters[c_j])\n",
        "\n",
        "                # Calculamos P(c_i, c_j)\n",
        "                frecuencia_conjunta = sum(\n",
        "                    1 for doc in corpus if any(token in palabras_i for token in doc) and any(token in palabras_j for token in doc)\n",
        "                )\n",
        "                if frecuencia_conjunta == 0:\n",
        "                    continue\n",
        "\n",
        "                prob_conjunta = frecuencia_conjunta / total_palabras\n",
        "                prob_conjunta = max(prob_conjunta, epsilon)\n",
        "                P_ci = max(P_c[c_i], epsilon)\n",
        "                P_cj = max(P_c[c_j], epsilon)\n",
        "\n",
        "                delta_I = prob_conjunta * math.log(prob_conjunta / (P_ci * P_cj))\n",
        "\n",
        "                if min_delta_I is None or delta_I < min_delta_I:\n",
        "                    min_delta_I = delta_I\n",
        "                    pair_to_merge = (c_i, c_j)\n",
        "\n",
        "        if pair_to_merge is None:\n",
        "            break  # No hay más fusiones posibles\n",
        "\n",
        "        c_i, c_j = pair_to_merge\n",
        "\n",
        "        # Fusionamos los clusters\n",
        "        clusters[c_i].extend(clusters.pop(c_j))\n",
        "\n",
        "        # Actualizamos P(c)\n",
        "        P_c[c_i] += P_c.pop(c_j)\n",
        "\n",
        "        print(f\"Fusión realizada. Clusters restantes: {len(clusters)}\")\n",
        "\n",
        "##########\n",
        "# Número de clusters deseado (100 clusters)\n",
        "num_clusters_deseado = 100\n",
        "\n",
        "fusionar_clusters_simple(clusters, P_c, corpus_entrenamiento, num_clusters_deseado)\n",
        "\n",
        "print(f\"Número final de clusters: {len(clusters)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXYuNNlVKNgg",
        "outputId": "f43fb3d7-8e6c-49b2-9a22-4ac74e7817c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número inicial de clusters: 229\n",
            "Probabilidades iniciales calculadas.\n",
            "Fusión realizada. Clusters restantes: 228\n",
            "Fusión realizada. Clusters restantes: 227\n",
            "Fusión realizada. Clusters restantes: 226\n",
            "Fusión realizada. Clusters restantes: 225\n",
            "Fusión realizada. Clusters restantes: 224\n",
            "Fusión realizada. Clusters restantes: 223\n",
            "Fusión realizada. Clusters restantes: 222\n",
            "Fusión realizada. Clusters restantes: 221\n",
            "Fusión realizada. Clusters restantes: 220\n",
            "Fusión realizada. Clusters restantes: 219\n",
            "Fusión realizada. Clusters restantes: 218\n",
            "Fusión realizada. Clusters restantes: 217\n",
            "Fusión realizada. Clusters restantes: 216\n",
            "Fusión realizada. Clusters restantes: 215\n",
            "Fusión realizada. Clusters restantes: 214\n",
            "Fusión realizada. Clusters restantes: 213\n",
            "Fusión realizada. Clusters restantes: 212\n",
            "Fusión realizada. Clusters restantes: 211\n",
            "Fusión realizada. Clusters restantes: 210\n",
            "Fusión realizada. Clusters restantes: 209\n",
            "Fusión realizada. Clusters restantes: 208\n",
            "Fusión realizada. Clusters restantes: 207\n",
            "Fusión realizada. Clusters restantes: 206\n",
            "Fusión realizada. Clusters restantes: 205\n",
            "Fusión realizada. Clusters restantes: 204\n",
            "Fusión realizada. Clusters restantes: 203\n",
            "Fusión realizada. Clusters restantes: 202\n",
            "Fusión realizada. Clusters restantes: 201\n",
            "Fusión realizada. Clusters restantes: 200\n",
            "Fusión realizada. Clusters restantes: 199\n",
            "Fusión realizada. Clusters restantes: 198\n",
            "Fusión realizada. Clusters restantes: 197\n",
            "Fusión realizada. Clusters restantes: 196\n",
            "Fusión realizada. Clusters restantes: 195\n",
            "Fusión realizada. Clusters restantes: 194\n",
            "Fusión realizada. Clusters restantes: 193\n",
            "Fusión realizada. Clusters restantes: 192\n",
            "Fusión realizada. Clusters restantes: 191\n",
            "Fusión realizada. Clusters restantes: 190\n",
            "Fusión realizada. Clusters restantes: 189\n",
            "Fusión realizada. Clusters restantes: 188\n",
            "Fusión realizada. Clusters restantes: 187\n",
            "Fusión realizada. Clusters restantes: 186\n",
            "Fusión realizada. Clusters restantes: 185\n",
            "Fusión realizada. Clusters restantes: 184\n",
            "Fusión realizada. Clusters restantes: 183\n",
            "Fusión realizada. Clusters restantes: 182\n",
            "Fusión realizada. Clusters restantes: 181\n",
            "Fusión realizada. Clusters restantes: 180\n",
            "Fusión realizada. Clusters restantes: 179\n",
            "Fusión realizada. Clusters restantes: 178\n",
            "Fusión realizada. Clusters restantes: 177\n",
            "Fusión realizada. Clusters restantes: 176\n",
            "Fusión realizada. Clusters restantes: 175\n",
            "Fusión realizada. Clusters restantes: 174\n",
            "Fusión realizada. Clusters restantes: 173\n",
            "Fusión realizada. Clusters restantes: 172\n",
            "Fusión realizada. Clusters restantes: 171\n",
            "Fusión realizada. Clusters restantes: 170\n",
            "Fusión realizada. Clusters restantes: 169\n",
            "Fusión realizada. Clusters restantes: 168\n",
            "Fusión realizada. Clusters restantes: 167\n",
            "Fusión realizada. Clusters restantes: 166\n",
            "Fusión realizada. Clusters restantes: 165\n",
            "Fusión realizada. Clusters restantes: 164\n",
            "Fusión realizada. Clusters restantes: 163\n",
            "Fusión realizada. Clusters restantes: 162\n",
            "Fusión realizada. Clusters restantes: 161\n",
            "Fusión realizada. Clusters restantes: 160\n",
            "Fusión realizada. Clusters restantes: 159\n",
            "Fusión realizada. Clusters restantes: 158\n",
            "Fusión realizada. Clusters restantes: 157\n",
            "Fusión realizada. Clusters restantes: 156\n",
            "Fusión realizada. Clusters restantes: 155\n",
            "Fusión realizada. Clusters restantes: 154\n",
            "Fusión realizada. Clusters restantes: 153\n",
            "Fusión realizada. Clusters restantes: 152\n",
            "Fusión realizada. Clusters restantes: 151\n",
            "Fusión realizada. Clusters restantes: 150\n",
            "Fusión realizada. Clusters restantes: 149\n",
            "Fusión realizada. Clusters restantes: 148\n",
            "Fusión realizada. Clusters restantes: 147\n",
            "Fusión realizada. Clusters restantes: 146\n",
            "Fusión realizada. Clusters restantes: 145\n",
            "Fusión realizada. Clusters restantes: 144\n",
            "Fusión realizada. Clusters restantes: 143\n",
            "Fusión realizada. Clusters restantes: 142\n",
            "Fusión realizada. Clusters restantes: 141\n",
            "Fusión realizada. Clusters restantes: 140\n",
            "Fusión realizada. Clusters restantes: 139\n",
            "Fusión realizada. Clusters restantes: 138\n",
            "Fusión realizada. Clusters restantes: 137\n",
            "Fusión realizada. Clusters restantes: 136\n",
            "Fusión realizada. Clusters restantes: 135\n",
            "Fusión realizada. Clusters restantes: 134\n",
            "Fusión realizada. Clusters restantes: 133\n",
            "Fusión realizada. Clusters restantes: 132\n",
            "Fusión realizada. Clusters restantes: 131\n",
            "Fusión realizada. Clusters restantes: 130\n",
            "Fusión realizada. Clusters restantes: 129\n",
            "Fusión realizada. Clusters restantes: 128\n",
            "Fusión realizada. Clusters restantes: 127\n",
            "Fusión realizada. Clusters restantes: 126\n",
            "Fusión realizada. Clusters restantes: 125\n",
            "Fusión realizada. Clusters restantes: 124\n",
            "Fusión realizada. Clusters restantes: 123\n",
            "Fusión realizada. Clusters restantes: 122\n",
            "Fusión realizada. Clusters restantes: 121\n",
            "Fusión realizada. Clusters restantes: 120\n",
            "Fusión realizada. Clusters restantes: 119\n",
            "Fusión realizada. Clusters restantes: 118\n",
            "Fusión realizada. Clusters restantes: 117\n",
            "Fusión realizada. Clusters restantes: 116\n",
            "Fusión realizada. Clusters restantes: 115\n",
            "Fusión realizada. Clusters restantes: 114\n",
            "Fusión realizada. Clusters restantes: 113\n",
            "Fusión realizada. Clusters restantes: 112\n",
            "Fusión realizada. Clusters restantes: 111\n",
            "Fusión realizada. Clusters restantes: 110\n",
            "Fusión realizada. Clusters restantes: 109\n",
            "Fusión realizada. Clusters restantes: 108\n",
            "Fusión realizada. Clusters restantes: 107\n",
            "Fusión realizada. Clusters restantes: 106\n",
            "Fusión realizada. Clusters restantes: 105\n",
            "Fusión realizada. Clusters restantes: 104\n",
            "Fusión realizada. Clusters restantes: 103\n",
            "Fusión realizada. Clusters restantes: 102\n",
            "Fusión realizada. Clusters restantes: 101\n",
            "Fusión realizada. Clusters restantes: 100\n",
            "Número final de clusters: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brown Clustering con mejoras en complejidad"
      ],
      "metadata": {
        "id": "V5hAQPsdJF71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Búsqueda de Fusión Óptima**\n",
        "\n",
        "Para determinar qué clusters deben fusionarse, se utiliza una cola de prioridad (`heap`) para almacenar los valores de `ΔI` para cada par de clusters. Esta métrica calcula la disminución en la información mutua cuando se consideran las probabilidades `P(c_i,c_j)` conjuntas realizando así más fácil la selección del par que menos afecta la información.\n",
        "\n",
        "\n",
        "**Fusión de Clusters**\n",
        "\n",
        "Se utiliza el `heap` para extraer los pares de clusters que tienen el menor `ΔI` y se realiza la fusión de dichos clusters. Esto se repite hasta alcanzar el número deseado de clusters `100`. Durante cada fusión, se recalculan las probabilidades para reflejar los cambios, y se actualiza el heap para la siguiente iteración.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z46Iu9qxJ-mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mejoras en Complejidad:**\n",
        "\n",
        "**Estructuras de Datos Eficientes**\n",
        "\n",
        "Se utilizan estructuras como el `heap` para manejar los pares de clusters y sus probabilidades, lo que permite una slección rápida de un par óptimo para la fusión. Esto reduce la complejidad computacional de seleccionar pares manualmente en cada iteración.\n",
        "\n",
        "**Técnicas de Poda**\n",
        "\n",
        " Al extraer elementos del heap con `heapq.heappop(heap)`, verificamos si los clusters correspondientes aún existen. Si alguno de los clusters ha sido fusionado previamente (y por lo tanto eliminado del diccionario clusters), simplemente continuamos al siguiente elemento del heap. De esta manera se realiza la poda."
      ],
      "metadata": {
        "id": "rfLgRtzXKE8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import collections\n",
        "import math\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "##########\n",
        "# Inicialización (Asignar cada palabra a su propio cluster)\n",
        "\n",
        "def inicializar_clusters(vocabulario: Dict[str, int]) -> Dict[int, List[str]]:\n",
        "    clusters = {i: [palabra] for palabra, i in vocabulario.items()}\n",
        "    return clusters\n",
        "\n",
        "clusters = inicializar_clusters(vocabulario)\n",
        "print(f\"Número inicial de clusters: {len(clusters)}\")\n",
        "\n",
        "##########\n",
        "# Cálculo de probabilidades P(c)\n",
        "\n",
        "def calcular_probabilidades_Pc(clusters: Dict[int, List[str]], corpus: List[List[str]]) -> Dict[int, float]:\n",
        "    total_palabras = sum(len(doc) for doc in corpus)\n",
        "    P_c = {}\n",
        "\n",
        "    # Contar frecuencia de clases\n",
        "    for c_id, palabras in clusters.items():\n",
        "        frecuencia_cluster = sum(1 for doc in corpus for token in doc if token in palabras)\n",
        "        P_c[c_id] = frecuencia_cluster / total_palabras\n",
        "\n",
        "    return P_c\n",
        "\n",
        "P_c = calcular_probabilidades_Pc(clusters, corpus_entrenamiento)\n",
        "print(\"Probabilidades iniciales calculadas.\")\n",
        "\n",
        "##########\n",
        "# Preparación de la cola de prioridad con ΔI(c_i, c_j)\n",
        "\n",
        "def inicializar_heap(P_c: Dict[int, float], clusters: Dict[int, List[str]], corpus: List[List[str]], epsilon: float = 1e-10) -> List[Tuple[float, int, int]]:\n",
        "    heap = []\n",
        "    total_palabras = sum(len(doc) for doc in corpus)\n",
        "    cluster_ids = list(clusters.keys())\n",
        "\n",
        "    # Calcular P(c_i, c_j) y ΔI para pares de clusters vecinos\n",
        "    for idx_i in range(len(cluster_ids)):\n",
        "        c_i = cluster_ids[idx_i]\n",
        "        palabras_i = set(clusters[c_i])\n",
        "        for idx_j in range(idx_i + 1, len(cluster_ids)):\n",
        "            c_j = cluster_ids[idx_j]\n",
        "            palabras_j = set(clusters[c_j])\n",
        "\n",
        "            # Calculamos P(c_i, c_j)\n",
        "            frecuencia_conjunta = sum(\n",
        "                1 for doc in corpus if any(token in palabras_i for token in doc) and any(token in palabras_j for token in doc)\n",
        "            )\n",
        "            if frecuencia_conjunta == 0:\n",
        "                continue\n",
        "\n",
        "            prob_conjunta = frecuencia_conjunta / total_palabras\n",
        "            prob_conjunta = max(prob_conjunta, epsilon)\n",
        "            P_ci = max(P_c[c_i], epsilon)\n",
        "            P_cj = max(P_c[c_j], epsilon)\n",
        "\n",
        "            delta_I = prob_conjunta * math.log(prob_conjunta / (P_ci * P_cj))\n",
        "\n",
        "            # Añadimos al heap\n",
        "            heapq.heappush(heap, (delta_I, c_i, c_j))\n",
        "\n",
        "    return heap\n",
        "\n",
        "heap = inicializar_heap(P_c, clusters, corpus_entrenamiento)\n",
        "print(\"Heap inicializado con los valores de ΔI.\")\n",
        "\n",
        "##########\n",
        "# Fusión de clusters con heap y poda\n",
        "\n",
        "def fusionar_clusters_heap(clusters: Dict[int, List[str]], heap: List[Tuple[float, int, int]], P_c: Dict[int, float], corpus: List[List[str]], num_clusters_deseado: int, epsilon: float = 1e-10):\n",
        "    total_palabras = sum(len(doc) for doc in corpus)\n",
        "\n",
        "    while len(clusters) > num_clusters_deseado:\n",
        "        if not heap:\n",
        "            break  # No hay más fusiones posibles\n",
        "\n",
        "        # Extraemos el par con el mínimo ΔI\n",
        "        delta_I, c_i, c_j = heapq.heappop(heap)\n",
        "\n",
        "        # Verificamos si los clusters aún existen (pueden haber sido fusionados)\n",
        "        if c_i not in clusters or c_j not in clusters:\n",
        "            continue\n",
        "\n",
        "        # Fusionamos los clusters\n",
        "        clusters[c_i].extend(clusters.pop(c_j))\n",
        "\n",
        "        # Actualizamos P(c)\n",
        "        P_c[c_i] += P_c.pop(c_j)\n",
        "\n",
        "        # Actualizamos el heap con los nuevos ΔI relacionados con c_i\n",
        "        for c_k in clusters:\n",
        "            if c_k == c_i:\n",
        "                continue\n",
        "\n",
        "            palabras_i = set(clusters[c_i])\n",
        "            palabras_k = set(clusters[c_k])\n",
        "\n",
        "            # Calculamos P(c_i, c_k)\n",
        "            frecuencia_conjunta = sum(\n",
        "                1 for doc in corpus if any(token in palabras_i for token in doc) and any(token in palabras_k for token in doc)\n",
        "            )\n",
        "            if frecuencia_conjunta == 0:\n",
        "                continue\n",
        "\n",
        "            prob_conjunta = frecuencia_conjunta / total_palabras\n",
        "            prob_conjunta = max(prob_conjunta, epsilon)\n",
        "            P_ck = max(P_c[c_k], epsilon)\n",
        "            P_ci = max(P_c[c_i], epsilon)\n",
        "\n",
        "            delta_I_nuevo = prob_conjunta * math.log(prob_conjunta / (P_ci * P_ck))\n",
        "\n",
        "            # Añadimos al heap el nuevo ΔI\n",
        "            heapq.heappush(heap, (delta_I_nuevo, c_i, c_k))\n",
        "\n",
        "        print(f\"Fusión realizada. Clusters restantes: {len(clusters)}\")\n",
        "\n",
        "##########\n",
        "# Número de clusters deseado (100 clusters)\n",
        "num_clusters_deseado = 100\n",
        "\n",
        "fusionar_clusters_heap(clusters, heap, P_c, corpus_entrenamiento, num_clusters_deseado)\n",
        "\n",
        "print(f\"Número final de clusters: {len(clusters)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vo3hPTXuEPt",
        "outputId": "b6aea4e5-f3bc-4fce-8379-5009e7455717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número inicial de clusters: 229\n",
            "Probabilidades iniciales calculadas.\n",
            "Heap inicializado con los valores de ΔI.\n",
            "Fusión realizada. Clusters restantes: 228\n",
            "Fusión realizada. Clusters restantes: 227\n",
            "Fusión realizada. Clusters restantes: 226\n",
            "Fusión realizada. Clusters restantes: 225\n",
            "Fusión realizada. Clusters restantes: 224\n",
            "Fusión realizada. Clusters restantes: 223\n",
            "Fusión realizada. Clusters restantes: 222\n",
            "Fusión realizada. Clusters restantes: 221\n",
            "Fusión realizada. Clusters restantes: 220\n",
            "Fusión realizada. Clusters restantes: 219\n",
            "Fusión realizada. Clusters restantes: 218\n",
            "Fusión realizada. Clusters restantes: 217\n",
            "Fusión realizada. Clusters restantes: 216\n",
            "Fusión realizada. Clusters restantes: 215\n",
            "Fusión realizada. Clusters restantes: 214\n",
            "Fusión realizada. Clusters restantes: 213\n",
            "Fusión realizada. Clusters restantes: 212\n",
            "Fusión realizada. Clusters restantes: 211\n",
            "Fusión realizada. Clusters restantes: 210\n",
            "Fusión realizada. Clusters restantes: 209\n",
            "Fusión realizada. Clusters restantes: 208\n",
            "Fusión realizada. Clusters restantes: 207\n",
            "Fusión realizada. Clusters restantes: 206\n",
            "Fusión realizada. Clusters restantes: 205\n",
            "Fusión realizada. Clusters restantes: 204\n",
            "Fusión realizada. Clusters restantes: 203\n",
            "Fusión realizada. Clusters restantes: 202\n",
            "Fusión realizada. Clusters restantes: 201\n",
            "Fusión realizada. Clusters restantes: 200\n",
            "Fusión realizada. Clusters restantes: 199\n",
            "Fusión realizada. Clusters restantes: 198\n",
            "Fusión realizada. Clusters restantes: 197\n",
            "Fusión realizada. Clusters restantes: 196\n",
            "Fusión realizada. Clusters restantes: 195\n",
            "Fusión realizada. Clusters restantes: 194\n",
            "Fusión realizada. Clusters restantes: 193\n",
            "Fusión realizada. Clusters restantes: 192\n",
            "Fusión realizada. Clusters restantes: 191\n",
            "Fusión realizada. Clusters restantes: 190\n",
            "Fusión realizada. Clusters restantes: 189\n",
            "Fusión realizada. Clusters restantes: 188\n",
            "Fusión realizada. Clusters restantes: 187\n",
            "Fusión realizada. Clusters restantes: 186\n",
            "Fusión realizada. Clusters restantes: 185\n",
            "Fusión realizada. Clusters restantes: 184\n",
            "Fusión realizada. Clusters restantes: 183\n",
            "Fusión realizada. Clusters restantes: 182\n",
            "Fusión realizada. Clusters restantes: 181\n",
            "Fusión realizada. Clusters restantes: 180\n",
            "Fusión realizada. Clusters restantes: 179\n",
            "Fusión realizada. Clusters restantes: 178\n",
            "Fusión realizada. Clusters restantes: 177\n",
            "Fusión realizada. Clusters restantes: 176\n",
            "Fusión realizada. Clusters restantes: 175\n",
            "Fusión realizada. Clusters restantes: 174\n",
            "Fusión realizada. Clusters restantes: 173\n",
            "Fusión realizada. Clusters restantes: 172\n",
            "Fusión realizada. Clusters restantes: 171\n",
            "Fusión realizada. Clusters restantes: 170\n",
            "Fusión realizada. Clusters restantes: 169\n",
            "Fusión realizada. Clusters restantes: 168\n",
            "Fusión realizada. Clusters restantes: 167\n",
            "Fusión realizada. Clusters restantes: 166\n",
            "Fusión realizada. Clusters restantes: 165\n",
            "Fusión realizada. Clusters restantes: 164\n",
            "Fusión realizada. Clusters restantes: 163\n",
            "Fusión realizada. Clusters restantes: 162\n",
            "Fusión realizada. Clusters restantes: 161\n",
            "Fusión realizada. Clusters restantes: 160\n",
            "Fusión realizada. Clusters restantes: 159\n",
            "Fusión realizada. Clusters restantes: 158\n",
            "Fusión realizada. Clusters restantes: 157\n",
            "Fusión realizada. Clusters restantes: 156\n",
            "Fusión realizada. Clusters restantes: 155\n",
            "Fusión realizada. Clusters restantes: 154\n",
            "Fusión realizada. Clusters restantes: 153\n",
            "Fusión realizada. Clusters restantes: 152\n",
            "Fusión realizada. Clusters restantes: 151\n",
            "Fusión realizada. Clusters restantes: 150\n",
            "Fusión realizada. Clusters restantes: 149\n",
            "Fusión realizada. Clusters restantes: 148\n",
            "Fusión realizada. Clusters restantes: 147\n",
            "Fusión realizada. Clusters restantes: 146\n",
            "Fusión realizada. Clusters restantes: 145\n",
            "Fusión realizada. Clusters restantes: 144\n",
            "Fusión realizada. Clusters restantes: 143\n",
            "Fusión realizada. Clusters restantes: 142\n",
            "Fusión realizada. Clusters restantes: 141\n",
            "Fusión realizada. Clusters restantes: 140\n",
            "Fusión realizada. Clusters restantes: 139\n",
            "Fusión realizada. Clusters restantes: 138\n",
            "Fusión realizada. Clusters restantes: 137\n",
            "Fusión realizada. Clusters restantes: 136\n",
            "Fusión realizada. Clusters restantes: 135\n",
            "Fusión realizada. Clusters restantes: 134\n",
            "Fusión realizada. Clusters restantes: 133\n",
            "Fusión realizada. Clusters restantes: 132\n",
            "Fusión realizada. Clusters restantes: 131\n",
            "Fusión realizada. Clusters restantes: 130\n",
            "Fusión realizada. Clusters restantes: 129\n",
            "Fusión realizada. Clusters restantes: 128\n",
            "Fusión realizada. Clusters restantes: 127\n",
            "Fusión realizada. Clusters restantes: 126\n",
            "Fusión realizada. Clusters restantes: 125\n",
            "Fusión realizada. Clusters restantes: 124\n",
            "Fusión realizada. Clusters restantes: 123\n",
            "Fusión realizada. Clusters restantes: 122\n",
            "Fusión realizada. Clusters restantes: 121\n",
            "Fusión realizada. Clusters restantes: 120\n",
            "Fusión realizada. Clusters restantes: 119\n",
            "Fusión realizada. Clusters restantes: 118\n",
            "Fusión realizada. Clusters restantes: 117\n",
            "Fusión realizada. Clusters restantes: 116\n",
            "Fusión realizada. Clusters restantes: 115\n",
            "Fusión realizada. Clusters restantes: 114\n",
            "Fusión realizada. Clusters restantes: 113\n",
            "Fusión realizada. Clusters restantes: 112\n",
            "Fusión realizada. Clusters restantes: 111\n",
            "Fusión realizada. Clusters restantes: 110\n",
            "Fusión realizada. Clusters restantes: 109\n",
            "Fusión realizada. Clusters restantes: 108\n",
            "Fusión realizada. Clusters restantes: 107\n",
            "Fusión realizada. Clusters restantes: 106\n",
            "Fusión realizada. Clusters restantes: 105\n",
            "Fusión realizada. Clusters restantes: 104\n",
            "Fusión realizada. Clusters restantes: 103\n",
            "Fusión realizada. Clusters restantes: 102\n",
            "Fusión realizada. Clusters restantes: 101\n",
            "Fusión realizada. Clusters restantes: 100\n",
            "Número final de clusters: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT**\n",
        "\n",
        "Al final, el número de clusters se reduce de 229 a 100, logrando una agrupación de términos basada en su contexto en el corpus.\n",
        "\n",
        "La agrupación eficiente de términos basada en su contexto se logra mediante el cálculo de la pérdida de información (`ΔI`) entre cada par de clusters. Cada término se inicia en su propio cluster, y en cada iteración, el algoritmo identifica el par de clusters cuya fusión causa la menor pérdida de información, lo que implica que estos clusters comparten un contexto similar."
      ],
      "metadata": {
        "id": "8j8f-lX3giib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Latent Semantic Analysis (LSA)"
      ],
      "metadata": {
        "id": "-E-AFpjnYZi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Semantic Analysis (LSA) simple\n"
      ],
      "metadata": {
        "id": "3PeJqPxx0Q3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construcción de la matriz término-documento con TF-IDF**\n",
        "\n",
        "Se construye la matriz `X` donde las filas representan términos y las columnas representan documentos. Los pesos en cada posición de la matriz se calculan utilizando `TF-IDF` (Frecuencia de Término - Frecuencia Inversa de Documento), lo que captura la importancia de un término en un documento específico en relación con el corpus completo.\n",
        "\n",
        "**Implementación de SVD usando NumPy**\n",
        "\n",
        "Se aplica la Descomposición en valores singulares tradicional a la matriz X utilizando la función `numpy.linalg.svd`. Esta función descompone `X` en tres matrices: `U`, `S` y `Vt`. Luego, se seleccionan las primeras `k` dimensiones más significativas para reducir la dimensionalidad del espacio.\n",
        "\n",
        "**Proyección en el espacio reducido**\n",
        "\n",
        "Las representaciones reducidas de términos y documentos se obtienen multiplicando las matrices resultantes `U_k` y `Vt_k` por `S_k`. Esto proyecta los términos y documentos en un espacio de menor dimensión definido por las `k` componentes principales, capturando las relaciones semánticas latentes en el corpus."
      ],
      "metadata": {
        "id": "IbkXI45meVNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "##########\n",
        "# Construcción de la matriz término-documento con TF-IDF\n",
        "\n",
        "def construir_matriz_termino_documento(corpus: List[List[str]], vocabulario: Dict[str, int]) -> np.ndarray:\n",
        "    num_docs = len(corpus)\n",
        "    num_terms = len(vocabulario)\n",
        "    X = np.zeros((num_terms, num_docs))\n",
        "\n",
        "    for j, documento in enumerate(corpus):\n",
        "        term_freq = {}\n",
        "        for token in documento:\n",
        "            if token in vocabulario:\n",
        "                idx = vocabulario[token]\n",
        "                term_freq[idx] = term_freq.get(idx, 0) + 1\n",
        "\n",
        "        for idx, freq in term_freq.items():\n",
        "            tf = freq / len(documento)\n",
        "            df = sum(1 for doc in corpus if vocabulario_inv[idx] in doc)\n",
        "            idf = math.log((num_docs) / (df + 1)) + 1  # Suavizado\n",
        "            X[idx, j] = tf * idf\n",
        "    return X\n",
        "\n",
        "# Crear el índice inverso del vocabulario\n",
        "vocabulario_inv = {idx: token for token, idx in vocabulario.items()}\n",
        "\n",
        "# Construir la matriz término-documento usando TF-IDF\n",
        "X = construir_matriz_termino_documento(corpus_entrenamiento, vocabulario)\n",
        "print(f\"Matriz término-documento construida con tamaño {X.shape}\")\n",
        "\n",
        "##########\n",
        "# Implementación de SVD usando numpy\n",
        "\n",
        "def aplicar_svd(X: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
        "    # Seleccionar solo las k dimensiones significativas\n",
        "    U_k = U[:, :k]\n",
        "    S_k = np.diag(S[:k])\n",
        "    Vt_k = Vt[:k, :]\n",
        "    return U_k, S_k, Vt_k\n",
        "\n",
        "# Selección de dimensiones significativas\n",
        "k = 5\n",
        "U_k, S_k, Vt_k = aplicar_svd(X, k)\n",
        "print(f\"SVD calculado con k={k} dimensiones significativas\")\n",
        "\n",
        "##########\n",
        "# Proyección en el espacio reducido\n",
        "\n",
        "# Obtener las representaciones reducidas para términos y documentos\n",
        "representacion_terminos = U_k @ S_k\n",
        "representacion_documentos = Vt_k.T @ S_k\n",
        "\n",
        "print(\"\\nRepresentaciones reducidas obtenidas para términos:\")\n",
        "print(representacion_terminos)\n",
        "\n",
        "print(\"\\nRepresentaciones reducidas obtenidas para documentos:\")\n",
        "print(representacion_documentos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lplZc7Bm0YB3",
        "outputId": "900386dc-54e0-4516-d1fe-8d2045535183"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz término-documento construida con tamaño (220, 1000)\n",
            "SVD calculado con k=5 dimensiones significativas\n",
            "\n",
            "Representaciones reducidas obtenidas para términos:\n",
            "[[ 3.75366086e-02 -6.14277986e-01  9.75220257e-31  1.48864005e+00\n",
            "  -3.01936432e+00]\n",
            " [ 9.79861628e-03 -1.47409896e-01 -4.87953903e-15  1.55969040e-01\n",
            "  -1.22584378e-01]\n",
            " [ 4.04140328e-01 -7.09139110e-01 -2.16292964e-16  5.64818442e-01\n",
            "  -1.77518752e+00]\n",
            " ...\n",
            " [ 7.30201345e-04 -3.03583804e-02 -2.65733526e-16  7.21819385e-02\n",
            "  -2.00799277e-01]\n",
            " [ 2.20834305e-04 -1.07535447e-02  1.63927046e-16  1.84451266e-02\n",
            "  -2.49167521e-02]\n",
            " [ 3.47809798e-04 -7.44887295e-03 -8.80303280e-16  2.12267513e-02\n",
            "  -5.81554367e-02]]\n",
            "\n",
            "Representaciones reducidas obtenidas para documentos:\n",
            "[[ 1.85732248e-02 -9.11829054e-02 -4.16862400e-17  1.15997132e-01\n",
            "  -3.07335490e-01]\n",
            " [ 4.00133757e-03 -5.27692320e-02 -6.15591836e-16  2.35633913e-02\n",
            "  -7.75161100e-02]\n",
            " [ 7.58068694e-04 -4.49662729e-03 -1.29709048e-16  2.35682930e-03\n",
            "  -1.61111052e-02]\n",
            " ...\n",
            " [ 8.22161305e-03 -4.36913370e-02  1.44365083e-15  1.37850928e-02\n",
            "  -1.93653781e-01]\n",
            " [ 8.16154048e-03 -1.05917966e+00 -5.72661184e-16 -1.07284105e-01\n",
            "   8.14581709e-02]\n",
            " [ 4.01087072e-03 -5.63742673e-02 -3.30289453e-16  4.09154860e-02\n",
            "  -1.81191556e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent Semantic Analysis (LSA) con implementaciones"
      ],
      "metadata": {
        "id": "hXK566LFPaEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construcción de la matriz término-documento**\n",
        "\n",
        "Se construye la matriz `x `donde las filas representan términos y las columnas representan documentos. Los pesos asignados a cada posición en la matriz son calculados utilizando `TF-IDF`, lo que captura la importancia de un término en un documento específico en relación con el corpus.\n",
        "\n",
        "**Implementación de SVD**\n",
        "\n",
        "Se implementa una versión de la descomposición en valores singulares (SVD) utilizando el método de potencias. Aproxima los valores y vectores singulares más grandes de la matriz `x`. Dado que SVD completo es complejo, la aproximación nos permite obtener las primeras k componentes principales.\n",
        "\n",
        "**Reducción de dimensionalidad**\n",
        "\n",
        "Después de obtener las matrices `U`, `S` y `Vt` de la descomposición, se seleccionan los primeros `k` valores singulares más grandes para reducir la dimensionalidad. Las matrices `U_k` y `V_k` multiplicadas por `S_k` proporcionan las representaciones reducidas de términos y documentos.\n",
        "\n",
        "**Proyección**\n",
        "\n",
        "Las representaciones reducidas `representacion_terminos` y `representacion_documentos` se obtienen proyectando los términos y documentos al espacio de menor dimensión definido por las `k` componentes principales. Esto captura las relaciones semánticas en el corpus."
      ],
      "metadata": {
        "id": "BEfdv8gWRYtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import collections\n",
        "from typing import List, Tuple, Dict\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "##########\n",
        "# Construcción de la matriz término-documento\n",
        "\n",
        "def construir_matriz_termino_documento(corpus: List[List[str]], vocabulario: Dict[str, int]) -> np.ndarray:\n",
        "    num_docs = len(corpus)\n",
        "    num_terms = len(vocabulario)\n",
        "    X = np.zeros((num_terms, num_docs))\n",
        "\n",
        "    for j, documento in enumerate(corpus):\n",
        "        term_freq = {}\n",
        "        for token in documento:\n",
        "            idx = vocabulario[token]\n",
        "            term_freq[idx] = term_freq.get(idx, 0) + 1\n",
        "        # Calcular TF-IDF\n",
        "        for idx, freq in term_freq.items():\n",
        "            tf = freq / len(documento)\n",
        "            # Calcular documento\n",
        "            df = sum(1 for doc in corpus if vocabulario_inv[idx] in doc)\n",
        "            idf = math.log((num_docs) / (df + 1)) + 1  # Suavizado\n",
        "            X[idx, j] = tf * idf\n",
        "    return X\n",
        "\n",
        "# Construir índice inverso del vocabulario\n",
        "vocabulario_inv = {idx: token for token, idx in vocabulario.items()}\n",
        "\n",
        "# Construir la matriz término-documento\n",
        "X = construir_matriz_termino_documento(corpus_entrenamiento, vocabulario)\n",
        "print(f\"Matriz término-documento construida con tamaño {X.shape}\")\n",
        "\n",
        "##########\n",
        "# Implementación de SVD\n",
        "\n",
        "def svd_potencias(X: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    # Implementación simplificada de SVD usando el método de potencias\n",
        "    m, n = X.shape\n",
        "    U = np.zeros((m, k))\n",
        "    S = np.zeros((k,))\n",
        "    Vt = np.zeros((k, n))\n",
        "\n",
        "    X_copy = X.copy()\n",
        "    for i in range(k):\n",
        "        # Inicializar vector aleatorio\n",
        "        v = np.random.rand(n)\n",
        "        v = v / np.linalg.norm(v)\n",
        "        # Método de potencias para obtener el valor singular máximo\n",
        "        for _ in range(20):  # Número de iteraciones\n",
        "            u = X_copy @ v\n",
        "            u = u / np.linalg.norm(u)\n",
        "            v = X_copy.T @ u\n",
        "            v = v / np.linalg.norm(v)\n",
        "        sigma = u.T @ X_copy @ v\n",
        "        U[:, i] = u\n",
        "        S[i] = sigma\n",
        "        Vt[i, :] = v.T\n",
        "        X_copy = X_copy - sigma * np.outer(u, v)\n",
        "    return U, S, Vt\n",
        "\n",
        "# Selección de dimensiones significativas\n",
        "k = 5\n",
        "U, S, Vt = svd_potencias(X, k)\n",
        "print(f\"SVD calculado con k={k} dimensiones significativas\")\n",
        "\n",
        "##########\n",
        "# Reducción de Dimensionalidad\n",
        "\n",
        "# Obtener las representaciones reducidas para términos y documentos\n",
        "U_k_S_k = U * S  # Matriz U_k multiplicada por S_k\n",
        "V_k_S_k = (Vt.T) * S  # Matriz V_k multiplicada por S_k\n",
        "\n",
        "##########\n",
        "# Proyección\n",
        "\n",
        "# Representaciones en espacio reducido\n",
        "representacion_terminos = U_k_S_k  # Representación de términos\n",
        "representacion_documentos = V_k_S_k  # Representación de documentos\n",
        "\n",
        "print(\"\\nRepresentaciones reducidas obtenidas para términos:\")\n",
        "print(representacion_terminos)\n",
        "\n",
        "print(\"\\nRepresentaciones reducidas obtenidas para documentos:\")\n",
        "print(representacion_documentos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzyGh0Z9QSSn",
        "outputId": "3a7680c9-6059-4264-fd48-2d3895baf06b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz término-documento construida con tamaño (220, 1000)\n",
            "SVD calculado con k=5 dimensiones significativas\n",
            "\n",
            "Representaciones reducidas obtenidas para términos:\n",
            "[[3.75380663e-02 6.32868445e-01 5.37491415e-01 1.48632608e+00\n",
            "  4.63727920e+00]\n",
            " [9.79896001e-03 1.49259495e-01 5.25386988e-02 1.49838412e-01\n",
            "  1.47753779e-01]\n",
            " [4.04141977e-01 7.16052354e-01 1.93358166e-01 5.67064391e-01\n",
            "  1.99467339e+00]\n",
            " ...\n",
            " [7.30273388e-04 3.12774358e-02 2.65147731e-02 7.36222428e-02\n",
            "  3.16384635e-01]\n",
            " [2.20859590e-04 1.09785172e-02 6.47424089e-03 1.80519012e-02\n",
            "  3.61658213e-02]\n",
            " [3.47827579e-04 7.71946581e-03 7.82719473e-03 2.16306605e-02\n",
            "  9.17572577e-02]]\n",
            "\n",
            "Representaciones reducidas obtenidas para documentos:\n",
            "[[ 1.85733760e-02  9.24688803e-02  3.87060248e-02  1.14797509e-01\n",
            "   3.94554752e-01]\n",
            " [ 4.00142426e-03  5.30222908e-02  7.06821257e-03  2.36107483e-02\n",
            "   1.03994841e-01]\n",
            " [ 7.58076091e-04  4.52418549e-03  7.82275438e-04  2.55288855e-03\n",
            "   2.31678418e-02]\n",
            " ...\n",
            " [ 8.22168475e-03  4.38547503e-02  4.35332736e-03  1.57057608e-02\n",
            "   1.89742746e-01]\n",
            " [ 8.16326947e-03  1.05763783e+00 -6.45904400e-02 -1.04303304e-01\n",
            "  -8.24687710e-02]\n",
            " [ 4.01096364e-03  5.68319468e-02  1.33631630e-02  4.16448698e-02\n",
            "   2.19909086e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT**\n",
        "\n",
        "La matriz término-documento construida tiene un tamaño de (220,1000), donde cada fila representa un término del vocabulario y cada columna corresponde a un documento. Después de aplicar la descomposición en valores singulares (SVD) y reducir a `K=5` dimensiones significativas.\n",
        "\n",
        "Obtenemos matrices de representaciones reducidas tanto para términos como para documentos. Las representaciones reducidas para términos y documentos nos dan un conjunto de vectores, donde cada vector describe la relación semántica de los términos y documentos con respecto a los componentes principales seleccionados."
      ],
      "metadata": {
        "id": "mg1DNukofX6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2vec"
      ],
      "metadata": {
        "id": "QZzd_Crhhezz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW simple"
      ],
      "metadata": {
        "id": "UcHpBvYk8d73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ventana de contexto**\n",
        "\n",
        "El código define una ventana de contexto de tamaño c, en este caso, 2 palabras alrededor de la palabra objetivo. Esto se implementa en la función `train_cbow`, donde para cada palabra en una oración, se seleccionan los índices de las palabras de contexto en un rango que se extiende `window_size` palabras a la izquierda y a la derecha de la palabra objetivo. Las palabras de contexto se identifican excluyendo la posición de la palabra objetivo, asegurando que solo se usen palabras cercanas para calcular el contexto. Esto permite capturar la relación entre la palabra objetivo y su contexto inmediato en el corpus.\n",
        "\n",
        "**Función de pérdida**\n",
        "\n",
        "La función de pérdida se basa en la pérdida negativa logarítmica, implementada mediante la función sigmoide para calcular la probabilidad de que una palabra en el contexto esté relacionada con la palabra objetivo. En cada paso de entrenamiento, se calcula el producto interno entre el embedding de la palabra objetivo y el vector promedio de las palabras de contexto (`h`). Luego, se calcula el error como `1 - sigmoid` para las palabras objetivo y `0 - sigmoid_neg` para las muestras negativas. Este enfoque permite al modelo maximizar la probabilidad de co-ocurrencia entre palabras en el contexto y reducirla para palabras negativas.\n",
        "\n",
        "**Optimización**\n",
        "\n",
        "La optimización se realiza mediante un algoritmo básico de descenso de gradiente. En cada iteración, el vector de salida `(W_out[target]`) y el vector de entrada (`h`) se actualizan utilizando la tasa de aprendizaje (`learning_rate`) multiplicada por el error. En la etapa de \"negative sampling\", palabras negativas aleatorias reciben actualizaciones en función de su error, utilizando una tasa de aprendizaje ajustada. Esto permite ajustar los embeddings de manera que reflejen mejor las relaciones observadas en el contexto, de este modo se asegura que las palabras relevantes para el contexto tengan embeddings similares."
      ],
      "metadata": {
        "id": "yaEDiZUivXj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# CBOW\n",
        "d = 50  # Dimension de los embeddings\n",
        "window_size = 2\n",
        "negative_samples = 2\n",
        "epocas = 1000\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Inicializamos los embeddings\n",
        "np.random.seed(42)  # Para reproducibilidad\n",
        "W_in = np.random.randn(len(vocabulario), d) * 0.01\n",
        "W_out = np.random.randn(len(vocabulario), d) * 0.01\n",
        "\n",
        "# Función de entrenamiento CBOW\n",
        "def train_cbow(corpus, W_in, W_out, epocas=1000, learning_rate=0.01):\n",
        "    for epoch in range(1, epocas + 1):\n",
        "        for sentence in corpus:\n",
        "            for i, palabra in enumerate(sentence):\n",
        "                context_indices = list(range(max(0, i - window_size), min(len(sentence), i + window_size + 1)))\n",
        "                context_indices.remove(i)  # Excluimos la palabra objetivo\n",
        "\n",
        "                # Obtener los índices de las palabras de contexto\n",
        "                context_words = [vocabulario[sentence[j]] for j in context_indices]\n",
        "                target = vocabulario[palabra]\n",
        "\n",
        "                # Vector de entrada: promedio de las palabras de contexto\n",
        "                h = np.mean([W_in[context_word] for context_word in context_words], axis=0)\n",
        "\n",
        "                # Actualizar con palabras objetivo positivas\n",
        "                z = np.dot(W_out[target], h)\n",
        "                sigmoid = 1 / (1 + np.exp(-z))\n",
        "                error = 1 - sigmoid\n",
        "                W_out[target] += learning_rate * error * h\n",
        "                h += learning_rate * error * W_out[target]\n",
        "\n",
        "                # Negative sampling\n",
        "                for _ in range(negative_samples):\n",
        "                    negative_word = random.randint(0, len(vocabulario)-1)\n",
        "                    while negative_word == target:\n",
        "                        negative_word = random.randint(0, len(vocabulario)-1)\n",
        "\n",
        "                    z_neg = np.dot(W_out[negative_word], h)\n",
        "                    sigmoid_neg = 1 / (1 + np.exp(-z_neg))\n",
        "                    error_neg = 0 - sigmoid_neg\n",
        "                    W_out[negative_word] += learning_rate * error_neg * h\n",
        "                    h += learning_rate * error_neg * W_out[negative_word]\n",
        "\n",
        "        # Progreso\n",
        "        if epoch % 200 == 0 or epoch == 1 or epoch == epocas:\n",
        "            print(f\"Época {epoch} completada\")\n",
        "\n",
        "    return W_in, W_out\n",
        "\n",
        "# Entrenamos el modelo CBOW\n",
        "W_in_trained_cbow, W_out_trained_cbow = train_cbow(corpus_entrenamiento, W_in, W_out, epocas, learning_rate)\n",
        "\n",
        "# Obtener una palabra aleatoria del vocabulario\n",
        "def get_random_word(vocabulario):\n",
        "    word = random.choice(list(vocabulario.keys()))\n",
        "    return word\n",
        "\n",
        "# Obtener el embedding de una palabra aleatoria del vocabulario\n",
        "def get_embedding_random(W_in):\n",
        "    word = get_random_word(vocabulario)\n",
        "    embedding = W_in[vocabulario[word]]\n",
        "    return word, embedding\n",
        "\n",
        "# Mostrar el embedding de una palabra aleatoria\n",
        "word, embedding_palabra = get_embedding_random(W_in_trained_cbow)\n",
        "print(f\"Embedding de '{word}': {embedding_palabra}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqEqI0o68ffp",
        "outputId": "07cc7f11-1c15-4a32-c089-b8d8192c6bd2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1 completada\n",
            "Época 200 completada\n",
            "Época 400 completada\n",
            "Época 600 completada\n",
            "Época 800 completada\n",
            "Época 1000 completada\n",
            "Embedding de 'algún': [ 0.00785185 -0.01777681  0.00714746 -0.00233724  0.00707458  0.00776932\n",
            "  0.01607915  0.01353454  0.01346572  0.0144367   0.00291379  0.00032185\n",
            "  0.00872817  0.00732081 -0.00309066  0.00404823 -0.00035096 -0.00853933\n",
            "  0.01290461  0.00055199  0.02048708 -0.01625492 -0.00556779  0.00032639\n",
            "  0.01137381  0.00399469  0.00310473 -0.01127264 -0.01156941  0.00322505\n",
            " -0.01364106 -0.00531878  0.00500917 -0.00982324  0.01605356  0.01223936\n",
            " -0.0157142  -0.00932371 -0.02285777 -0.00782591  0.00766388 -0.00413895\n",
            "  0.00460797 -0.01522334  0.00797975 -0.00555109  0.0001217  -0.00092875\n",
            " -0.01271549 -0.01153084]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT**\n",
        "\n",
        "La línea Embedding de `'algún'` muestra el embedding de una palabra específica del vocabulario. Este embedding es un vector de 50 dimensiones, ya que la dimensión `d` del embedding se configuró en 50 al inicio del código. Cada valor en el vector representa una dimensión de la representación semántica de la palabra `'algún'`, aprendida durante el entrenamiento. Este vector captura la relación de `'algún'` con otras palabras en el corpus en función de su contexto, permitiendo que el modelo use esta información para tareas de predicción o similitud de palabras.\n"
      ],
      "metadata": {
        "id": "eZvWaAAUz49d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-Gram con negative sampling simple"
      ],
      "metadata": {
        "id": "nA2TTQcx89R7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicción de contexto**\n",
        "\n",
        "En el modelo Skip-Gram, para cada palabra objetivo `(target)`, el código predice las palabras de contexto dentro de una ventana de tamaño` window_size`. Esto se logra generando un rango de índices alrededor de la palabra objetivo. La lista `context_indices` contiene los índices de las palabras dentro de la ventana de contexto, excluyendo el índice de la palabra objetivo en sí misma. Luego, el código itera sobre cada palabra en `context_indices` y la usa como contexto, calculando una predicción con el vector `W_in[target]` y el vector `W_out[context]` para ajustar sus pesos mediante descenso de gradiente.\n",
        "\n",
        "**Negative sampling**\n",
        "\n",
        "El código utiliza `negative sampling` que introduce palabras negativas (no en el contexto) durante el aprendizaje. En cada iteración, el código selecciona palabras negativas aleatorias de vocabulario, se asegura de que no coincidan con la palabra de contexto actual. Esta selección se realiza con la línea `neg = random.randint(0, len(vocabulario)-1)`. Luego, el código calcula el error para estas palabras negativas y ajusta los embeddings de `W_in` y `W_out` usando este error, refuerza la separación entre palabras no relacionadas.\n",
        "\n",
        "**Función de pérdida**\n",
        "\n",
        "La función de pérdida en este código utiliza la salida de la función sigmoide para medir la similitud entre la palabra objetivo y las palabras de contexto (pares positivos) o las palabras negativas (pares negativos). Para los pares positivos, se calcula `sigmoid = 1 / (1 + np.exp(-z))` donde `z` es el producto punto entre el embedding de la palabra objetivo y el de la palabra de contexto. El error se define como `1 - sigmoid`, y se usa para ajustar los vectores. Para las palabras negativas, el cálculo es similar, pero el error se define como `0 - sigmoid_neg`, reduce la probabilidad de relación entre la palabra objetivo y las palabras negativas."
      ],
      "metadata": {
        "id": "-_HpXSxB3deJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Parámetros\n",
        "d = 50  # Dimension de los embeddings\n",
        "window_size = 2\n",
        "negative_samples = 2\n",
        "epocas = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Normalizar vectores para evitar explison de gradiente\n",
        "def normalize(vec):\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / norm if norm > 0 else vec\n",
        "\n",
        "# Inicializar los embeddings (W_in y W_out)\n",
        "np.random.seed(42)\n",
        "W_in = np.random.randn(len(vocabulario), d) * 0.01\n",
        "W_out = np.random.randn(len(vocabulario), d) * 0.01\n",
        "\n",
        "# Función de entrenamiento para Skip-Gram\n",
        "def train_skipgram(corpus, W_in, W_out, epocas=1000, learning_rate=0.001):\n",
        "    for epoch in range(1, epocas + 1):\n",
        "        for sentence in corpus:\n",
        "            for i, palabra in enumerate(sentence):\n",
        "                target = vocabulario[palabra]\n",
        "                # Definir el contexto dentro del window_size\n",
        "                context_indices = list(range(max(0, i - window_size), min(len(sentence), i + window_size + 1)))\n",
        "                context_indices.remove(i)  # Remover el target del contexto\n",
        "                for j in context_indices:\n",
        "                    context = vocabulario[sentence[j]]\n",
        "\n",
        "                    # Positivo\n",
        "                    z = np.dot(W_out[context], W_in[target])\n",
        "                    sigmoid = 1 / (1 + np.exp(-z))\n",
        "                    error = 1 - sigmoid\n",
        "                    # Actualizar W_out y W_in para el ejemplo positivo\n",
        "                    W_out[context] += learning_rate * error * W_in[target]\n",
        "                    W_in[target] += learning_rate * error * W_out[context]\n",
        "\n",
        "                    # Negativos\n",
        "                    for _ in range(negative_samples):\n",
        "                        neg = random.randint(0, len(vocabulario)-1)\n",
        "                        while neg == context:\n",
        "                            neg = random.randint(0, len(vocabulario)-1)\n",
        "                        z_neg = np.dot(W_out[neg], W_in[target])\n",
        "                        sigmoid_neg = 1 / (1 + np.exp(-z_neg))\n",
        "                        error_neg = 0 - sigmoid_neg\n",
        "                        W_out[neg] += learning_rate * error_neg * W_in[target]\n",
        "                        W_in[target] += learning_rate * error_neg * W_out[neg]\n",
        "\n",
        "                    # Normalizar vectores para evitar que se vuelvan demasiado grandes\n",
        "                    W_in[target] = normalize(W_in[target])\n",
        "                    W_out[context] = normalize(W_out[context])\n",
        "\n",
        "        # Progreso\n",
        "        if epoch % 200 == 0 or epoch == 1 or epoch == epocas:\n",
        "            print(f\"Época {epoch} completada\")\n",
        "\n",
        "    return W_in, W_out\n",
        "\n",
        "# Entrenamos el modelo Skip-Gram con Negative Sampling\n",
        "W_in_trained_sg, W_out_trained_sg = train_skipgram(corpus_entrenamiento, W_in, W_out, epocas, learning_rate)\n",
        "\n",
        "# Obtener una palabra aleatoria del vocabulario\n",
        "def get_random_word(vocabulario):\n",
        "    word = random.choice(list(vocabulario.keys()))\n",
        "    return word\n",
        "\n",
        "# Obtener el embedding de una palabra aleatoria del vocabulario\n",
        "def get_embedding_random(W_in):\n",
        "    word = get_random_word(vocabulario)\n",
        "    embedding = W_in[vocabulario[word]]\n",
        "    return word, embedding\n",
        "\n",
        "# Mostrar el embedding de una palabra\n",
        "word, embedding_palabra_sg = get_embedding_random(W_in_trained_sg)\n",
        "print(f\"Embedding de '{word}' (Skip-Gram): {embedding_palabra_sg}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXYYiKOV89fp",
        "outputId": "ba6271cc-b2e0-40dc-cc95-022fb0635d75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1 completada\n",
            "Época 200 completada\n",
            "Época 400 completada\n",
            "Época 600 completada\n",
            "Época 800 completada\n",
            "Época 1000 completada\n",
            "Embedding de 'alemán' (Skip-Gram): [-0.22641096  0.04032551  0.08926692 -0.04716543  0.03258699  0.07874471\n",
            " -0.17789721  0.12780811 -0.12820677  0.08469878  0.13018643  0.1257329\n",
            "  0.16722604  0.02180892 -0.00720884 -0.21150646 -0.03950202 -0.08072195\n",
            " -0.04290617  0.31031471  0.03337721 -0.04912377  0.16993618  0.00385315\n",
            "  0.37868471 -0.19414078 -0.1400535   0.11660446  0.11522243  0.15947553\n",
            " -0.12347388  0.02654927  0.06612638  0.00372539 -0.07902223 -0.06957144\n",
            "  0.0131463  -0.25117155 -0.07918373 -0.06516045 -0.002367    0.10841236\n",
            "  0.12498371 -0.32530566  0.0370282  -0.03793557 -0.29913918 -0.14300072\n",
            "  0.04758757  0.08380323]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT**\n",
        "\n",
        "Embedding de `'alemán'`: Después del entrenamiento, el código genera el vector de embedding de la palabra `'alemán'`. Los valores en el vector incluyen tanto números positivos como negativos, lo cual es esperado debido al uso de técnicas como negative sampling, que contribuyen a ajustar los embeddings en función de las palabras de contexto y las palabras negativas, estableciendo relaciones semánticas."
      ],
      "metadata": {
        "id": "hWy3wP5j5ZDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CBOW** se centra en predecir la palabra objetivo a partir del contexto, mientras que **Skip-Gram ** intenta predecir el contexto a partir de la palabra objetivo. Ambos usan negative sampling para reducir el cálculo y mejorar el entrenamiento."
      ],
      "metadata": {
        "id": "Dz2F5Pna51u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe"
      ],
      "metadata": {
        "id": "O3suu7Yo8eYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Construcción de la Matriz de co-ocurrencia X**\n",
        "\n",
        "La función `construir_matriz_coocurrencia` recorre el corpus y crea una matriz de co-ocurrencia donde cada entrada representa el número de veces que una palabra aparece en el contexto de otra dentro de una ventana determinada (tamaño `window_size`). Esta matriz captura la relación de proximidad entre palabras en el corpus.\n",
        "\n",
        "**Inicialización de vectores y sesgos**\n",
        "\n",
        " Los vectores de palabras (`W`) y de contexto (`W_contexto`) se inicializan aleatoriamente utilizando una distribución normal. Además, se inicializan los sesgos `b` y `b_contexto` como vectores de ceros para cada palabra en el vocabulario.\n",
        "\n",
        "**Optimización de la función de costo**\n",
        "\n",
        "La función `entrenar_glove` implementa el entrenamiento del modelo GloVe mediante una función de costo que minimiza la diferencia entre el producto escalar de los vectores de palabra y contexto, más sus sesgos, y el logaritmo de las frecuencias de co-ocurrencia. Se utiliza un gradiente descendente estocástico para actualizar los vectores y sesgos en cada iteración, con el objetivo de minimizar el error.\n",
        "\n",
        "**Iteración hasta convergencia**\n",
        "\n",
        "El entrenamiento se realiza durante un número predefinido de épocas (`epocas`), actualizando los parámetros hasta que se alcanza un mínimo del error total. Durante el proceso, el error total se imprime en ciertas épocas para registrar la convergencia del modelo y asegurarse de que la función de costo está disminuyendo."
      ],
      "metadata": {
        "id": "J9-fF37tH4pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# GloVe\n",
        "d = 50  # Dimensión de los embeddings\n",
        "epocas = 350\n",
        "learning_rate = 0.05\n",
        "x_max = 100  # Parámetro de suavizado\n",
        "alpha = 0.75  # Exponente en la función de ponderación\n",
        "\n",
        "# Construcción de la matriz de co-ocurrencia\n",
        "def construir_matriz_coocurrencia(corpus, vocabulario, window_size=2):\n",
        "    matriz_coocurrencia = np.zeros((len(vocabulario), len(vocabulario)))\n",
        "    for documento in corpus:\n",
        "        for i, palabra in enumerate(documento):\n",
        "            palabra_indice = vocabulario[palabra]\n",
        "            # Contexto a la izquierda y derecha\n",
        "            for j in range(max(i - window_size, 0), min(i + window_size + 1, len(documento))):\n",
        "                if i != j:\n",
        "                    contexto_indice = vocabulario[documento[j]]\n",
        "                    matriz_coocurrencia[palabra_indice, contexto_indice] += 1\n",
        "    return matriz_coocurrencia\n",
        "\n",
        "# Inicialización de vectores y sesgos\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(len(vocabulario), d) * 0.01  # Vectores de palabras (objetivo)\n",
        "W_contexto = np.random.randn(len(vocabulario), d) * 0.01  # Vectores de contexto\n",
        "b = np.zeros(len(vocabulario))  # Sesgos para palabras objetivo\n",
        "b_contexto = np.zeros(len(vocabulario))  # Sesgos para palabras de contexto\n",
        "\n",
        "# Función de ponderación\n",
        "def funcion_ponderacion(x, x_max, alpha):\n",
        "    return (x / x_max)**alpha if x < x_max else 1\n",
        "\n",
        "# Entrenamiento de GloVe\n",
        "def entrenar_glove(matriz_coocurrencia, W, W_contexto, b, b_contexto, epocas, learning_rate, x_max, alpha):\n",
        "    for epoch in range(epocas):\n",
        "        total_error = 0\n",
        "        # Iterar sobre cada par de palabras\n",
        "        for i in range(len(vocabulario)):\n",
        "            for j in range(len(vocabulario)):\n",
        "                if matriz_coocurrencia[i, j] > 0:\n",
        "                    X_ij = matriz_coocurrencia[i, j]\n",
        "                    # Calcular el error y la función de pérdida\n",
        "                    w_dot = np.dot(W[i], W_contexto[j])\n",
        "                    log_Xij = np.log(X_ij)\n",
        "                    peso = funcion_ponderacion(X_ij, x_max, alpha)\n",
        "                    error = (w_dot + b[i] + b_contexto[j] - log_Xij)\n",
        "                    total_error += peso * (error ** 2)\n",
        "\n",
        "                    # Gradientes\n",
        "                    grad_w = peso * error * W_contexto[j]\n",
        "                    grad_w_contexto = peso * error * W[i]\n",
        "                    grad_b = peso * error\n",
        "                    grad_b_contexto = peso * error\n",
        "\n",
        "                    # Actualizar los parámetros\n",
        "                    W[i] -= learning_rate * grad_w\n",
        "                    W_contexto[j] -= learning_rate * grad_w_contexto\n",
        "                    b[i] -= learning_rate * grad_b\n",
        "                    b_contexto[j] -= learning_rate * grad_b_contexto\n",
        "\n",
        "        # Progreso\n",
        "        if (epoch + 1) % 100 == 0 or epoch == 0:\n",
        "            print(f\"Época {epoch + 1}/{epocas}, Error total: {total_error:.4f}\")\n",
        "\n",
        "    return W, W_contexto\n",
        "\n",
        "# Construir matriz de co-ocurrencia\n",
        "matriz_coocurrencia = construir_matriz_coocurrencia(corpus_entrenamiento, vocabulario)\n",
        "\n",
        "# Entrenar el modelo GloVe\n",
        "W_trained, W_contexto_trained = entrenar_glove(matriz_coocurrencia, W, W_contexto, b, b_contexto, epocas, learning_rate, x_max, alpha)\n",
        "\n",
        "# Obtener el embedding de una palabra aleatoria del vocabulario\n",
        "def get_embedding_random(W):\n",
        "    word = random.choice(list(vocabulario.keys()))\n",
        "    embedding = W[vocabulario[word]]\n",
        "    return word, embedding\n",
        "\n",
        "# Mostrar el embedding de una palabra aleatoria\n",
        "word, embedding_palabra = get_embedding_random(W_trained)\n",
        "print(f\"Embedding de '{word}' (GloVe): {embedding_palabra}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NZFfYWZ8ep8",
        "outputId": "a828d313-1a6d-4cb8-ae4e-94726eff49b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1/350, Error total: 291.0738\n",
            "Época 100/350, Error total: 44.3040\n",
            "Época 200/350, Error total: 19.9556\n",
            "Época 300/350, Error total: 10.5266\n",
            "Embedding de 'social' (GloVe): [ 0.09853697  0.06702795  0.14756727 -0.07771305  0.28116965 -0.03490453\n",
            "  0.17554624 -0.16920621  0.12402475 -0.22440128  0.15073892 -0.28913077\n",
            "  0.13841251  0.2651067  -0.12441293  0.16481013 -0.07080443 -0.02971872\n",
            "  0.1696008  -0.10228858 -0.01757578 -0.11689146 -0.10164319 -0.08348371\n",
            " -0.42876538  0.1207345   0.04650726 -0.04542867 -0.11074363 -0.23176622\n",
            " -0.03405249 -0.34122812 -0.13890086 -0.16995003 -0.02052767  0.19202377\n",
            "  0.11511954 -0.05574901  0.17863603 -0.25131119 -0.01812615 -0.04267015\n",
            " -0.16474377  0.43447716 -0.10171589  0.19114515  0.06633759  0.27699309\n",
            " -0.17502346 -0.02310334]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTPUT**\n",
        "\n",
        "Inicialmente, el error total es alto (291.07 en la primera época), pero disminuye a 10.52 en la época 300, mostrando una convergencia hacia valores más bajos.\n",
        "\n",
        "El último mensaje muestra el embedding generado para la palabra `'social'`. Este vector tiene 50 dimensiones (`d = 50`), y cada valor representa una característica que captura relaciones semánticas con otras palabras. Los valores incluyen tanto positivos como negativos, reflejando una combinación de interacciones en el contexto de la palabra `'social'`."
      ],
      "metadata": {
        "id": "XgBGpzveDuUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 4"
      ],
      "metadata": {
        "id": "BNtRZVEwPofF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NO IMPLEMENTADO"
      ],
      "metadata": {
        "id": "Cu5SVK05WEIW"
      }
    }
  ]
}